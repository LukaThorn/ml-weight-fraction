{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Readme\" data-toc-modified-id=\"Readme-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Readme</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tips\" data-toc-modified-id=\"Tips-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Tips</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Classes\" data-toc-modified-id=\"Classes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Classes</a></span></li><li><span><a href=\"#Pre-processing-functions\" data-toc-modified-id=\"Pre-processing-functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pre-processing functions</a></span></li><li><span><a href=\"#Model-functions\" data-toc-modified-id=\"Model-functions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model functions</a></span></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Export</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source file for classes and functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For writing error messages\n",
    "    * https://stackoverflow.com/questions/16451514/returning-error-string-from-a-function-in-python\n",
    "* List of objects: dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T18:44:46.422265Z",
     "start_time": "2019-07-15T18:44:45.453092Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T18:44:48.274284Z",
     "start_time": "2019-07-15T18:44:48.266989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure directory exists.\n"
     ]
    }
   ],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "PROJECT_SAVE_DIR = \"figs\"\n",
    "\n",
    "if not (os.path.isdir(PROJECT_ROOT_DIR+'/'+PROJECT_SAVE_DIR)):\n",
    "    print('Figure directory did not exist, creating now.')\n",
    "    os.mkdir(PROJECT_ROOT_DIR+'/'+PROJECT_SAVE_DIR)\n",
    "else:\n",
    "    print('Figure directory exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T18:44:50.019949Z",
     "start_time": "2019-07-15T18:44:50.005960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"./data/ping.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To enable a specified sound to play\n",
    "from IPython.display import Audio\n",
    "sound_file = './data/ping.wav'\n",
    "\n",
    "# Option to play sound at the end of a function with a long run time\n",
    "Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T18:46:07.172494Z",
     "start_time": "2019-07-15T18:46:07.142330Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in ENM feature data\n",
    "X_enm = pd.read_csv(\"./data/ENM-preprocessed-feats.csv\", sep='\\t', \n",
    "                    header='infer', index_col=0)\n",
    "\n",
    "# Read in ENM labels (maximum_weight_fraction)\n",
    "y_enm = pd.read_csv(\"./data/ENM-clean.csv\", sep=',', \n",
    "                    header='infer', usecols=[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T18:46:10.753181Z",
     "start_time": "2019-07-15T18:46:10.747249Z"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    \"\"\"\n",
    "    Option to suppress print output.\n",
    "    Source:\n",
    "    https://stackoverflow.com/questions/8391411/suppress-calls-to-print-python\n",
    "    \"\"\"\n",
    "    import os, sys\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T18:46:12.807530Z",
     "start_time": "2019-07-15T18:46:11.628346Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "    \"\"\"\n",
    "    Set up grid search across multiple estimators, pipelines.\n",
    "    By David Bastista:\n",
    "    http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "    \"\"\"\n",
    "    #from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    cv=10\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" \n",
    "                             % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=cv, n_jobs=1, verbose=1, \n",
    "            scoring='accuracy', refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T18:46:13.584641Z",
     "start_time": "2019-07-15T18:46:13.579586Z"
    }
   },
   "outputs": [],
   "source": [
    "def bins(row):\n",
    "    \"\"\"\n",
    "    Assign weight fractions (continuous) to bins (int).\n",
    "    Class ranges are slightly different from those used by Isaacs et al. 2016.\n",
    "    \"\"\"\n",
    "    if row['maximum_weight_fraction'] <= 0.002:\n",
    "        val = 0 # low\n",
    "    elif row['maximum_weight_fraction'] > 0.05:\n",
    "        val = 2 # high\n",
    "    else:\n",
    "        val = 1 # medium\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T18:46:14.247125Z",
     "start_time": "2019-07-15T18:46:14.235176Z"
    }
   },
   "outputs": [],
   "source": [
    "bin_enm = np.asarray(y_enm.apply(bins, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T18:46:15.271118Z",
     "start_time": "2019-07-15T18:46:15.259587Z"
    }
   },
   "outputs": [],
   "source": [
    "def bar_graph_bins(label_data,\n",
    "                   data_composition):\n",
    "    \"\"\"\n",
    "    This function creates a bar graph of weight fraction bins and prints the \n",
    "    count and frequency for each.\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    label_data: int array of shape [n,]\n",
    "        Dataframe containing binned wf data\n",
    "    data_composition: string\n",
    "        Describes the chemical composition of label_data \n",
    "        for use in the plot title; e.g., `ENM`, `Organics`   \n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Find the count, frequency of WF bins\n",
    "    unique, counts = np.unique(label_data, return_counts=True)\n",
    "    wf_distrib = dict(zip(unique, counts))\n",
    "    freq = []\n",
    "    for i in counts:\n",
    "        percent = (i/np.sum(counts)).round(2)\n",
    "        freq.append(percent)\n",
    "\n",
    "    # Plot\n",
    "    plt.bar(range(len(wf_distrib)), list(wf_distrib.values()), align='center')\n",
    "    plt.xticks(range(len(wf_distrib)), list(['low','medium','high']))\n",
    "    plt.title('Frequency of %s Weight Fraction Bins' % data_composition)\n",
    "    plt.show()\n",
    "    \n",
    "    print('Label bin: ', unique)\n",
    "    print('Count    : ', counts)\n",
    "    print('Frequency: ', freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:21:16.150843Z",
     "start_time": "2019-07-15T20:21:16.130112Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_conf_matrix(cm, \n",
    "                     classes, \n",
    "                     normalize=False, \n",
    "                     title='Confusion Matrix', \n",
    "                     cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    Adapted from:\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import itertools\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.gcf().subplots_adjust(bottom=0.2)\n",
    "    plt.ylabel('True weight fraction')\n",
    "    plt.xlabel('Predicted weight fraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:21:16.907476Z",
     "start_time": "2019-07-15T20:21:16.895864Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_param_opt(param_grid, test_scores, scoring): \n",
    "    \n",
    "    \"\"\"\n",
    "    Optional plot of validation score vs classifier parameter(s). For use \n",
    "    after running parameter optimization with GridSearchCV.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    def convert_log_scale(n_set, n_label):\n",
    "        log_dif = np.abs(np.log10(max(n_set)) - np.log10(min(n_set)))\n",
    "        if log_dif > 3:\n",
    "            n_set = np.log10(n_set)\n",
    "            n_label = ('log_10(%s)' % n_label)    \n",
    "        return n_set, n_label\n",
    "\n",
    "    params = {k.split(\"__\")[1]: v for k, v in param_grid.items()}\n",
    "    param1_label = list(params.keys())[0]\n",
    "    param1_set = list(params.values())[0]\n",
    "    param1_set, param1_label = convert_log_scale(param1_set, param1_label)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    if len(param_grid.keys()) == 1:\n",
    "        plt.plot(param1_set, test_scores, 'k.-', ms=8, lw=2)\n",
    "        plt.title('%s vs %s' % (scoring.title(), param1_label))\n",
    "        plt.xlabel(param1_label)\n",
    "        plt.ylabel(scoring.title())\n",
    "        plt.xticks(np.arange(min(param1_set), max(param1_set) + 2, 2))\n",
    "    elif len(param_grid.keys()) == 2:\n",
    "        param2_label = list(params.keys())[1]\n",
    "        param2_set = list(param_grid.values())[1]\n",
    "        param2_set, param2_label = convert_log_scale(param2_set, param2_label)\n",
    "        test_scores = np.reshape(test_scores, newshape=[-1, len(param2_set)])\n",
    "        plt.contourf(param2_set, param1_set, test_scores)\n",
    "        plt.title('%s Contours Over Parameter Grid' % scoring.title())\n",
    "        plt.xlabel(param2_label)\n",
    "        plt.ylabel(param1_label)\n",
    "        plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:21:17.767839Z",
     "start_time": "2019-07-15T20:21:17.752938Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feat_impt(feature_names, \n",
    "                   importances, \n",
    "                   variances=None, \n",
    "                   save_fig_name=None,\n",
    "                   combo_impt=False):\n",
    "    \"\"\"\n",
    "    This function uses results from an rfc as input to plot feature importance.\n",
    "    Here, the rfc determines importance using what is known as gini importance \n",
    "    or mean decrease impurity. Includes option to combine features into more \n",
    "    easily interpretable groups.\n",
    "    \n",
    "    References:\n",
    "    https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined\n",
    "    https://matplotlib.org/examples/api/barchart_demo.html\n",
    "    https://stackoverflow.com/questions/28931224/adding-value-labels-on-a-matplotlib-bar-chart\n",
    "    \"\"\" \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # (Optional) Sum importance by feature group\n",
    "    if combo_impt:\n",
    "        feature_names = ['chemProperties', 'functions', 'productCategories', \n",
    "                               'productType', 'productMatrix']\n",
    "        importances = np.asarray([np.sum(importances[0:4]), \n",
    "                                  np.sum(importances[4:20]), \n",
    "                                  np.sum(importances[20:27]), \n",
    "                                  np.sum(importances[27:36]), \n",
    "                                  np.sum(importances[36:])])\n",
    "        # (Optional) Sum variance by feature group\n",
    "        if np.all(variances != None):\n",
    "            variances = np.asarray([np.sum(variances[0:4]), \n",
    "                                    np.sum(variances[4:20]),\n",
    "                                    np.sum(variances[20:27]),\n",
    "                                    np.sum(variances[27:36]),\n",
    "                                    np.sum(variances[36:])])\n",
    "    \n",
    "    indices = np.argsort(importances)\n",
    "    \n",
    "    # (Optional) Add error bars\n",
    "    if np.all(variances != None):\n",
    "        err_bars = np.sqrt(variances)\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.grid(True)\n",
    "        ax.barh(range(len(indices)), importances[indices], \n",
    "                 xerr=err_bars[indices], capsize=3, align='center')\n",
    "    else: \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.barh(range(len(indices)), importances[indices], align='center')\n",
    "    \n",
    "    # Add grid lines\n",
    "    plt.grid(False)\n",
    "    ax.set_xticks(np.arange(0, np.amax(importances)+0.1, 0.05))\n",
    "    ax.xaxis.grid(color='silver')\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Label parts of plot\n",
    "    ax.set_title('Feature Importance')\n",
    "    ax.set_xlabel('Relative Importance')\n",
    "    ax.set_yticks(np.arange(len(feature_names)))\n",
    "    ax.set_yticklabels([feature_names[i] for i in indices])\n",
    "    # Add importance value labels at the end of bars\n",
    "    if variances is None:\n",
    "        for rect in ax.patches:\n",
    "            # Get X and Y placement of label from rect\n",
    "            x_value = rect.get_width()\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{:.2f}\".format(x_value)\n",
    "            # Create annotation\n",
    "            plt.annotate(\n",
    "                label,\n",
    "                (x_value, y_value),         # Place label at end of the bar\n",
    "                xytext=(4, 0),              # Horizontally shift label\n",
    "                textcoords=\"offset points\", # Interpret `xytext` as offset\n",
    "                va='center', ha='left')\n",
    "    \n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    if combo_impt: fig.set_size_inches(10, 6)\n",
    "    else: fig.set_size_inches(10, 10)\n",
    "    if np.all(save_fig_name != None):\n",
    "        fig.savefig('./figs/feature_importance_%s.png' % save_fig_name, \n",
    "                   bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:29:16.055597Z",
     "start_time": "2019-07-15T20:29:16.024268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to optimize, execute and evaluate a classifier using CV\n",
    "from numpy import random\n",
    "\n",
    "def model_opt_exe(classifier, \n",
    "                  X_training, \n",
    "                  y_training, \n",
    "                  X_testing=X_enm, \n",
    "                  y_testing=bin_enm, \n",
    "                  seed=random.randint(1,100),\n",
    "                  save_fig_name=None, \n",
    "                  match_group=None, \n",
    "                  show_opt_plot=False, \n",
    "                  show_feat_impt=False, \n",
    "                  show_cnf_matrix=False, \n",
    "                  param_grid=None):\n",
    "    \"\"\"\n",
    "    This function consists of three parts:\n",
    "    1) Optimize the parameters for a classifier, either SVC-RBF or RFC;     \n",
    "    2) Fit model pipeline to training data using optimized parameters and \n",
    "    stratified k-fold cross validation;\n",
    "    3) Execute the optimized model and summarize its accuracy in a confusion \n",
    "    matrix broken down by WF bins. Formatted confusion matrices are saved as \n",
    "    .png files.\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    classifier: string ('svc' or 'rfc')\n",
    "        The classifier to use in the pipeline; 'svc' refers to an SVC-RBF\n",
    "    X_training: pandas data frame\n",
    "        Feature data frame to train the model on\n",
    "    y_training: pandas data frame\n",
    "        WF (labels) data frame to train the model on\n",
    "    X_testing: pandas data frame (default=X_enm)\n",
    "        Feature data frame to test the best model on\n",
    "    y_testing: pandas data frame (default=y_enm)\n",
    "        WF (labels) data frame to test the best model on   \n",
    "    seed: int (default=random.randint(1,100))\n",
    "        Option to set the seed for CV\n",
    "    save_fig_name: string (default=None)\n",
    "        A unique string used at the end of confusion matrix and feature \n",
    "        importance (rfc-only) file names for exporting the figures as .png; \n",
    "        `None` indicates that no figures should be saved\n",
    "    match_group: array of int (default=None)\n",
    "        The array of ENM indices that augmented data were matched to; \n",
    "        applicable only to dfs with matching augmentation; prevents data leaks\n",
    "    show_opt_plot: bool (default=False)\n",
    "        `True` will plot accuracy as contour lines on the specified parameter \n",
    "        grid (svc) or a line plot of accuracy vs n_trees (rfc)\n",
    "    show_cnf_matrix: bool (default=False)\n",
    "        `True` results in matrix graphics being printed as output\n",
    "    param_grid: dict (default=None)\n",
    "        See param_grid for sklearn's GridSearchCV\n",
    "    \"\"\"     \n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn import model_selection\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from numpy import random\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # =====PART 1=====\n",
    "    # Optimize parameters\n",
    "    \n",
    "    # Define pipeline options for parameter optimization\n",
    "    rfc = RandomForestClassifier(class_weight='balanced', \n",
    "                                 random_state=seed)\n",
    "    svc = SVC(kernel='rbf', \n",
    "              class_weight='balanced',  # balances weights of WF bins\n",
    "              random_state=seed)\n",
    "    if classifier=='rfc':               # set pipeline for RFC\n",
    "        pipe = Pipeline([\n",
    "            ('scale', MinMaxScaler()),  # normalization from 0 to 1\n",
    "            ('estimator', rfc)          # use RFC algorithm specified above\n",
    "        ])\n",
    "    else:                               # set pipeline for SVC-RBF\n",
    "        pipe = Pipeline([\n",
    "            ('scale', MinMaxScaler()),\n",
    "            ('estimator', svc)\n",
    "        ])\n",
    "\n",
    "    # Set what kind of stratified k-fold CV to run\n",
    "    num_folds = 10\n",
    "    # When matching augmentation was NOT used, run normal stratified k-fold CV\n",
    "    if np.all(match_group == None):\n",
    "        cv = num_folds\n",
    "    # When matching augmentation was used, keep each group of matched data \n",
    "    # samples together based on ENM index (match_group) when splitting data \n",
    "    # into folds so that there is no data leakage during CV\n",
    "    else: \n",
    "        gfk = GroupKFold(n_splits=num_folds)\n",
    "        gfk.random_state = seed\n",
    "        cv = gfk.split(X_training, y_training, match_group)\n",
    "\n",
    "    # Find best algorithm parameters by searching over a grid using the CV\n",
    "    # and pipeline conditions specified above\n",
    "    n_jobs = 3\n",
    "    scoring = 'accuracy'\n",
    "    grid_search = GridSearchCV(pipe, \n",
    "                               param_grid, \n",
    "                               cv=cv, \n",
    "                               scoring=scoring, \n",
    "                               n_jobs=n_jobs, \n",
    "                               pre_dispatch=2*n_jobs)\n",
    "    grid_search.fit(X_training, y_training)\n",
    "    \n",
    "    # Retrieve accuracy scores for all grid search settings\n",
    "    test_scores = grid_search.cv_results_.get('mean_test_score')\n",
    "    \n",
    "    # If optimization plotting is set as True, use plot_param_opt function\n",
    "    # to plot a 2D or contour plot to visualize accuracy \"hot spots\"\n",
    "    if show_opt_plot:\n",
    "        plot_param_opt(param_grid, test_scores, scoring)\n",
    "    \n",
    "    # Retrieve best parameters from grid search (using list comprehension)\n",
    "    best_params = {k.split(\"__\")[1]: v \n",
    "                   for k, v in grid_search.best_params_.items()}\n",
    "    \n",
    "    # Print best accuracy and parameter values\n",
    "    print('K-fold CV random state:\\t', seed)\n",
    "    print('Best fold %s:\\t%.4f' % (scoring, grid_search.best_score_))\n",
    "    for k, v in grid_search.best_params_.items(): \n",
    "        print('Best %s:\\t%.2e' % (k, v))\n",
    "    \n",
    "    # =====PART 2=====\n",
    "    # Fit optimized pipeline to training data\n",
    "    \n",
    "    # RFC pipeline                    \n",
    "    if classifier == 'rfc':\n",
    "        rfc = RandomForestClassifier(class_weight='balanced', \n",
    "                                     random_state=seed, \n",
    "                                     **best_params) # use optimized parameters\n",
    "        pipe = Pipeline([\n",
    "            ('scale', MinMaxScaler()),\n",
    "            ('estimator', rfc)\n",
    "        ])\n",
    "        pipe.fit(X_training, y_training)        # fit pipeline to training data\n",
    "        importances = rfc.feature_importances_  # get feature impt. from fit\n",
    "        \n",
    "        # Option to plot feature importance (RFC only)\n",
    "        if show_feat_impt:\n",
    "            feature_names = X_training.columns.values\n",
    "            plot_feat_impt(feature_names, importances, save_fig_name)      \n",
    "    \n",
    "    # SVC pipeline\n",
    "    else:\n",
    "        svc = SVC(kernel='rbf', \n",
    "                  class_weight='balanced', \n",
    "                  random_state=seed, \n",
    "                  **best_params)                # use optimized parameters\n",
    "        pipe = Pipeline([\n",
    "            ('scale', MinMaxScaler()),\n",
    "            ('estimator', svc)\n",
    "        ])\n",
    "        pipe.fit(X_training,y_training)\n",
    "    \n",
    "    # =====PART 3=====\n",
    "    # Model execution and performance summary\n",
    "    \n",
    "    X = np.array(X_testing)\n",
    "    y = np.array(y_testing)\n",
    "    \n",
    "    # Set CV as ~leave-one-out (based on sample size of the smallest WF bin)\n",
    "    kfold = model_selection.StratifiedKFold(n_splits=17, # smallest bin size\n",
    "                                            shuffle=True, \n",
    "                                            random_state=seed)\n",
    "    \n",
    "    # Placeholder matrix of accuracies averaged across CV folds\n",
    "    cnf_matrix = np.zeros([3,3]) # 3 \"true\" vs 3 \"predicted\" WF bins\n",
    "    \n",
    "    # Run fitted pipeline using CV conditions defined above               \n",
    "    for train_index, test_index in kfold.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index] # split test data\n",
    "        y_train, y_test = y[train_index], y[test_index] # into folds\n",
    "        y_enm_predict = pipe.predict(X_test)\n",
    "        y[test_index] = y_enm_predict\n",
    "        # Write accuracy results to confusion matrix\n",
    "        cnf_matrix += confusion_matrix(y_test, y_enm_predict)\n",
    "    cnf_matrix = cnf_matrix.astype(np.int)\n",
    "    np.set_printoptions(precision=2)\n",
    "    class_names = [\"low\",\"mid\",\"high\"]\n",
    "\n",
    "    # Plot and save non-normalized confusion matrix\n",
    "    fig = plt.figure()\n",
    "    plot_conf_matrix(cnf_matrix, classes=class_names, normalize=False)\n",
    "    if np.all(save_fig_name != None):\n",
    "        fig.savefig('./figs/confusion_notnorm_%s.png' % save_fig_name)\n",
    "    if not show_cnf_matrix: plt.close(fig)\n",
    "\n",
    "    # Plot and save normalized confusion matrix\n",
    "    fig = plt.figure()\n",
    "    plot_conf_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                         title='Normalized Confusion Matrix')\n",
    "    if np.all(save_fig_name != None):\n",
    "        fig.savefig('./figs/confusion_norm_%s.png' % save_fig_name)\n",
    "    if not show_cnf_matrix: plt.close(fig)\n",
    "    \n",
    "    # Calculate the average normalized accuracy across all bins\n",
    "    cm_norm = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:,np.newaxis]\n",
    "    avg_norm = (cm_norm[0,0] + cm_norm[1,1] + cm_norm[2,2]) / 3\n",
    "    print('Average normalized accuracy: ', avg_norm)\n",
    "    \n",
    "    # Play sound when done running\n",
    "    display(Audio(url=sound_file, autoplay=True))\n",
    "    \n",
    "    # Set output based on chosen classifier\n",
    "    if classifier == 'rfc':\n",
    "        return avg_norm, importances\n",
    "    else:\n",
    "        return avg_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:29:39.105827Z",
     "start_time": "2019-07-15T20:29:39.093996Z"
    }
   },
   "outputs": [],
   "source": [
    "def multi_trials(num_trials, \n",
    "                 model_params):\n",
    "    \"\"\"\n",
    "    This function repeats model_opt_exe for a specified number of trials and\n",
    "    provides summary statistics. Returns avg mean (scalar), avg stdev \n",
    "    (scalar), and optionally, for RFC, arrays for average feature importance \n",
    "    and variance.\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    num_trials: int\n",
    "        The number of times to repeat\n",
    "    model_params: dict\n",
    "        A dictionary of parameters to run model_opt_exe \n",
    "    \"\"\"  \n",
    "    seed_set = np.random.choice(np.arange(1,101), \n",
    "                                size=num_trials, \n",
    "                                replace=False)\n",
    "    with HiddenPrints():   # Hides function output for all the trials\n",
    "        rs = []\n",
    "        for seed in seed_set:\n",
    "            model_params['seed'] = seed\n",
    "            # Apply all-in-one function that optimizes and executes model\n",
    "            rs_row = model_opt_exe(**model_params)\n",
    "            rs.append(rs_row)\n",
    "    # For RFC, write accuracy and feature importance results\n",
    "    if model_params['classifier'] == 'rfc':\n",
    "        results_accu = np.array([x for x, _ in rs]) # list comprehension\n",
    "        results_impt = np.array([y for _, y in rs])\n",
    "        avg_impt = results_impt.mean(axis=0)        # average importance\n",
    "        var_impt = results_impt.var(axis=0)         # variance of importance\n",
    "    # For SVC-RBF, only write accuracy results\n",
    "    else:\n",
    "        results_accu = np.array([x for x in rs])\n",
    "       \n",
    "    mu = results_accu.mean()   # average accuracy across trials\n",
    "    sigma = results_accu.std() # standard deviation\n",
    "    \n",
    "    # Print summary statistics across trials\n",
    "    print(\"Avg accuracy:    \", mu)\n",
    "    print(\"Median accuracy: \", np.median(results_accu))\n",
    "    print(\"StdDev accuracy: \", sigma)\n",
    "    print(\"Numer of trials: \", num_trials)\n",
    "    #print(\"Results: \", results_accu)\n",
    "    \n",
    "    # Play sound when done running\n",
    "    display(Audio(url=sound_file, autoplay=True))\n",
    "    \n",
    "    # Set output based on chosen classifier\n",
    "    if model_params['classifier'] == 'rfc':\n",
    "        return mu, sigma, avg_impt, var_impt\n",
    "    else: \n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:29:46.893515Z",
     "start_time": "2019-07-15T20:29:44.086226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook functions.ipynb to script\n",
      "[NbConvertApp] Writing 24846 bytes to functions.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
