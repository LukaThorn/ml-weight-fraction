{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the source file for classes and functions used throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T14:36:32.434644Z",
     "iopub.status.busy": "2021-11-05T14:36:32.432248Z",
     "iopub.status.idle": "2021-11-05T14:36:32.689780Z",
     "shell.execute_reply": "2021-11-05T14:36:32.689203Z",
     "shell.execute_reply.started": "2021-11-05T14:36:32.434593Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %load_ext lab_black\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Couldn't load Black autoformatter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:40.730556Z",
     "start_time": "2020-09-14T00:07:39.596248Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-05T14:36:33.559896Z",
     "iopub.status.busy": "2021-11-05T14:36:33.559425Z",
     "iopub.status.idle": "2021-11-05T14:36:34.504201Z",
     "shell.execute_reply": "2021-11-05T14:36:34.503331Z",
     "shell.execute_reply.started": "2021-11-05T14:36:33.559852Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.display import display\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:41.251584Z",
     "start_time": "2020-09-14T00:07:41.242136Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-05T14:36:35.528235Z",
     "iopub.status.busy": "2021-11-05T14:36:35.527700Z",
     "iopub.status.idle": "2021-11-05T14:36:35.558345Z",
     "shell.execute_reply": "2021-11-05T14:36:35.551903Z",
     "shell.execute_reply.started": "2021-11-05T14:36:35.528175Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure directory exists.\n"
     ]
    }
   ],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "PROJECT_SAVE_DIR = \"figs\"\n",
    "\n",
    "if not (os.path.isdir(PROJECT_ROOT_DIR + \"/\" + PROJECT_SAVE_DIR)):\n",
    "    print(\"Figure directory did not exist, creating now.\")\n",
    "    os.mkdir(PROJECT_ROOT_DIR + \"/\" + PROJECT_SAVE_DIR)\n",
    "else:\n",
    "    print(\"Figure directory exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:42.188636Z",
     "start_time": "2020-09-14T00:07:42.015127Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-05T14:36:36.264160Z",
     "iopub.status.busy": "2021-11-05T14:36:36.263924Z",
     "iopub.status.idle": "2021-11-05T14:36:36.499455Z",
     "shell.execute_reply": "2021-11-05T14:36:36.498385Z",
     "shell.execute_reply.started": "2021-11-05T14:36:36.264127Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in target (ENM) model feature data\n",
    "X_enm = pd.read_csv(\n",
    "    \"./data/ENM-preprocessed-feats.csv\", sep=\"\\t\", header=\"infer\", index_col=0\n",
    ")\n",
    "\n",
    "# Read in source (organics) model feature data\n",
    "X_source = pd.read_csv(\n",
    "    \"./data/organics-preprocessed-feats.csv\", sep=\"\\t\", header=\"infer\", index_col=0\n",
    ")\n",
    "\n",
    "# Read in ENM labels (maximum_weight_fraction)\n",
    "y_enm = pd.read_csv(\"./data/ENM-clean.csv\", sep=\",\", header=\"infer\", usecols=[4])\n",
    "\n",
    "# Read in organics labels (maximum_weight_fraction)\n",
    "y_source = pd.read_csv(\n",
    "    \"./data/organics-preprocessed-WF.csv\", sep=\"\\t\", header=\"infer\", index_col=0\n",
    ")\n",
    "y_source.index = X_source.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility classes, functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:48.397494Z",
     "start_time": "2020-09-14T00:07:48.384565Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-08T07:51:44.745227Z",
     "iopub.status.busy": "2021-09-08T07:51:44.744966Z",
     "iopub.status.idle": "2021-09-08T07:51:44.761886Z",
     "shell.execute_reply": "2021-09-08T07:51:44.760833Z",
     "shell.execute_reply.started": "2021-09-08T07:51:44.745199Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def savepdf(fig, name):\n",
    "    \"\"\"Save figures as .pdf files\"\"\"\n",
    "\n",
    "    # Adjust matplotlib settings so text is editable in PDFs\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    plt.rcParams[\"font.sans-serif\"] = \"Helvetica\"\n",
    "    plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "    plt.rcParams[\"ps.fonttype\"] = 42\n",
    "    # Save PDF in figure directory\n",
    "    fig.savefig(\n",
    "        PROJECT_ROOT_DIR + \"/\" + PROJECT_SAVE_DIR + \"/\" + name + \".pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        transparent=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T07:51:45.560630Z",
     "iopub.status.busy": "2021-09-08T07:51:45.559870Z",
     "iopub.status.idle": "2021-09-08T07:51:45.597000Z",
     "shell.execute_reply": "2021-09-08T07:51:45.595453Z",
     "shell.execute_reply.started": "2021-09-08T07:51:45.560586Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_latex(df, file_name, tex_label=\"INSERT_LABEL\", caption=\"INSERT_CAPTION\"):\n",
    "    \"\"\"\n",
    "    Exports DataFrame as text file suitable for LaTeX\n",
    "\n",
    "    Pre-formatted for the tabularx LaTeX package.\n",
    "    \"\"\"\n",
    "\n",
    "    # Where to save the results\n",
    "    directory_root = \".\"\n",
    "    directory_sub = \"results\"\n",
    "\n",
    "    if not (os.path.isdir(directory_root + \"/\" + directory_sub)):\n",
    "        print(\"Results directory did not exist, creating now.\")\n",
    "        os.mkdir(directory_root + \"/\" + directory_sub)\n",
    "    else:\n",
    "        print(\"Results directory exists.\")\n",
    "\n",
    "    file_path = directory_root + \"/\" + directory_sub + \"/\" + file_name + \".txt\"\n",
    "    print(file_path)\n",
    "\n",
    "    # Write to text file\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[htb]\\n\")\n",
    "        f.write(\n",
    "            \"\\\\begin{tabularx}{\\\\textwidth}{\" + \" \".join(\"X\" * len(df.columns)) + \"}\\n\"\n",
    "        )\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\n",
    "            \" & \".join([\"\\\\textbf{\" + str(col) + \"}\" for col in df.columns]) + \" \\\\\\\\\\n\"\n",
    "        )\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        for i, row in df.iterrows():\n",
    "            f.write(\" & \".join([str(x) for x in row.values]) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\end{tabularx}\\n\")\n",
    "        f.write(\"\\\\caption{\" + caption + \"}\\n\")\n",
    "        f.write(\"\\\\label{tab:\" + tex_label + \"}\\n\")\n",
    "        f.write(\"\\\\end{table}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:16:10.768505Z",
     "iopub.status.busy": "2021-06-09T09:16:10.767799Z",
     "iopub.status.idle": "2021-06-09T09:16:11.340377Z",
     "shell.execute_reply": "2021-06-09T09:16:11.339679Z",
     "shell.execute_reply.started": "2021-06-09T09:16:10.768407Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "def norm_test(data):\n",
    "    \"\"\"\n",
    "    Shapiro-Wilk test for normality\n",
    "    \n",
    "    Source:\n",
    "    https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/\n",
    "    \"\"\"\n",
    "    # Normality test\n",
    "    stat, p = shapiro(data)\n",
    "    print(\"Statistics=%.3f, p=%.3f\" % (stat, p))\n",
    "    # Interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print(\"Sample looks Gaussian (fail to reject H0)\")\n",
    "    else:\n",
    "        print(\"Sample does not look Gaussian (reject H0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:44.171123Z",
     "start_time": "2020-09-14T00:07:44.161208Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-08T07:51:47.439137Z",
     "iopub.status.busy": "2021-09-08T07:51:47.438618Z",
     "iopub.status.idle": "2021-09-08T07:51:47.453605Z",
     "shell.execute_reply": "2021-09-08T07:51:47.452613Z",
     "shell.execute_reply.started": "2021-09-08T07:51:47.439088Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    \"\"\"\n",
    "    Option to suppress print output.\n",
    "\n",
    "    Source:\n",
    "    https://stackoverflow.com/questions/8391411/suppress-calls-to-print-python\n",
    "    \"\"\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T07:51:54.157047Z",
     "iopub.status.busy": "2021-09-08T07:51:54.156265Z",
     "iopub.status.idle": "2021-09-08T07:51:54.169887Z",
     "shell.execute_reply": "2021-09-08T07:51:54.168147Z",
     "shell.execute_reply.started": "2021-09-08T07:51:54.156994Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bins(row):\n",
    "    \"\"\"Assign weight fractions (continuous) to bins (int)\n",
    "\n",
    "    Class ranges are different from those used by Isaacs et al. 2016.\n",
    "    \"\"\"\n",
    "\n",
    "    if row[\"maximum_weight_fraction\"] <= 0.01:\n",
    "        val = 0  # low\n",
    "    elif row[\"maximum_weight_fraction\"] > 0.10:\n",
    "        val = 2  # high\n",
    "    else:\n",
    "        val = 1  # medium\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:08:00.905642Z",
     "start_time": "2020-09-14T00:08:00.345259Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-08T07:51:54.795568Z",
     "iopub.status.busy": "2021-09-08T07:51:54.795276Z",
     "iopub.status.idle": "2021-09-08T07:51:55.076977Z",
     "shell.execute_reply": "2021-09-08T07:51:55.075939Z",
     "shell.execute_reply.started": "2021-09-08T07:51:54.795536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_enm = np.asarray(y_enm.apply(bins, axis=1))\n",
    "bin_source = np.asarray(y_source.apply(bins, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:16:10.056190Z",
     "iopub.status.busy": "2021-06-09T09:16:10.055405Z",
     "iopub.status.idle": "2021-06-09T09:16:10.159839Z",
     "shell.execute_reply": "2021-06-09T09:16:10.159088Z",
     "shell.execute_reply.started": "2021-06-09T09:16:10.056107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pca_precheck(X, n_components, thres_btm=0.75, thres_top=0.90):\n",
    "    \"\"\"List PCA components for hyperparameterization\n",
    "\n",
    "    Find a reasonable range of n_components to try during hyperparameterization.\n",
    "    Returns a list of integers.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    X : DataFrame\n",
    "        Feature data for dimension reduction\n",
    "    n_components : int\n",
    "        A generous number of PCA components to test\n",
    "    thres_btm : float (default=0.75)\n",
    "        The minimum cumulative explained variance threshold to aim for; a fraction\n",
    "    thres_top : float (default=0.85)\n",
    "        The maximum cumulative explained variance threshold to aim for\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn import decomposition\n",
    "\n",
    "    # Scale the data first (e.g., chemical properties) from 0 to 1\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    pca.fit(X_scaled)\n",
    "    cum_evr = np.cumsum(pca.explained_variance_ratio_)\n",
    "    print(cum_evr)\n",
    "\n",
    "    # Figure out number of components to achieve desired cumulative explained variance\n",
    "    component_list = np.where((cum_evr > thres_btm) & (cum_evr < thres_top))[0] + 1\n",
    "    component_list = component_list.tolist()\n",
    "\n",
    "    if len(component_list) == 0:\n",
    "        component_list = [\n",
    "            next(i[0] for i in enumerate(cum_evr) if i[1] > thres_top) + 1\n",
    "        ]\n",
    "\n",
    "    # Plot just to double check / visualize\n",
    "    fig = plt.figure()\n",
    "    xi = np.arange(1, n_components + 1, step=1)\n",
    "    plt.plot(xi, cum_evr, \".-\", label=\"pca\")\n",
    "    plt.plot([0, n_components], [thres_btm, thres_btm], \"k\", label=thres_btm)\n",
    "    plt.plot([0, n_components], [thres_top, thres_top], \"r\", label=thres_top)\n",
    "    plt.xlabel(\"Coefficient Number\")\n",
    "    plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(component_list)\n",
    "    return component_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:08:01.617402Z",
     "start_time": "2020-09-14T00:08:01.601209Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-08T07:58:41.876520Z",
     "iopub.status.busy": "2021-09-08T07:58:41.873885Z",
     "iopub.status.idle": "2021-09-08T07:58:41.965424Z",
     "shell.execute_reply": "2021-09-08T07:58:41.964441Z",
     "shell.execute_reply.started": "2021-09-08T07:58:41.876399Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bar_graph_bins(label_data, data_composition):\n",
    "    \"\"\"Bar graph of weight fraction bins\n",
    "\n",
    "    Create a bar graph of weight fraction bins and print the\n",
    "    count and frequency for each.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    label_data : int array of shape [n,]\n",
    "        Dataframe containing binned wf data\n",
    "    data_composition : string\n",
    "        Describes the chemical composition of label_data\n",
    "        for use in the plot title; e.g., `ENM`, `Organics`\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Find the count, frequency of WF bins\n",
    "    unique, counts = np.unique(label_data, return_counts=True)\n",
    "    wf_distrib = dict(zip(unique, counts))\n",
    "    freq = []\n",
    "    for i in counts:\n",
    "        percent = (i / np.sum(counts)).round(2)\n",
    "        freq.append(percent)\n",
    "    bin_names = [\"low\", \"medium\", \"high\"]\n",
    "    if len(unique) == 4:\n",
    "        bin_names = [\"xlow\"] + bin_names\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(range(len(wf_distrib)), list(wf_distrib.values()), align=\"center\")\n",
    "    ax.set_xticks(range(len(wf_distrib)))\n",
    "    ax.set_xticklabels(list(bin_names))\n",
    "    ax.set_xlabel(\"Weight fraction bin\")\n",
    "    ax.set_ylabel(\"Frequency of observations\")\n",
    "    savepdf(fig, \"histogram_bins_%s\" % data_composition.lower().replace(\" \", \"\"))\n",
    "    #ax.title(\"Frequency of %s Weight Fraction Bins\" % data_composition)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Label bin: \", unique)\n",
    "    print(\"Count    : \", counts)\n",
    "    print(\"Frequency: \", freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:16:11.642848Z",
     "iopub.status.busy": "2021-06-09T09:16:11.642385Z",
     "iopub.status.idle": "2021-06-09T09:16:11.773629Z",
     "shell.execute_reply": "2021-06-09T09:16:11.772936Z",
     "shell.execute_reply.started": "2021-06-09T09:16:11.642790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for plotting piecharts\n",
    "def plot_piechart(\n",
    "    data, feat_subset_prefix, save_fig_name, figsize=[3, 2.5], labels=None\n",
    "):\n",
    "\n",
    "    # Aesthetic settings\n",
    "    font_body = {\"fontsize\": 7, \"fontname\": \"Helvetica\"}\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    plt.rcParams[\"font.sans-serif\"] = \"Helvetica\"\n",
    "    my_colors = [\n",
    "        \"tab:blue\",  # sky blue\n",
    "        \"#E59400\",  # orange\n",
    "        \"tab:purple\",  # purple-blue\n",
    "        \"#C0504D\",  # red\n",
    "        \"tab:olive\",\n",
    "        \"#323299\",  # dark blue\n",
    "        \"#7a307a\",  # violet\n",
    "        \"seagreen\",\n",
    "        \"sienna\",\n",
    "        \"gold\",\n",
    "    ]\n",
    "\n",
    "    # Define data and labels for plotting\n",
    "    feat_subset = [f for f in data.columns if feat_subset_prefix in f]\n",
    "    if len(feat_subset) == 1:\n",
    "        pos_labels = np.count_nonzero(data[feat_subset])\n",
    "        values = [(pos_labels), (len(data[feat_subset]) - pos_labels)]\n",
    "        if labels is None:\n",
    "            labels = [feat_subset[0].split(\"_\")[1], \"other\"]\n",
    "        else:\n",
    "            labels = labels\n",
    "\n",
    "    else:\n",
    "        values = data[feat_subset].sum(axis=0)\n",
    "        labels = [f.split(\"_\")[1] for f in feat_subset]\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.pie(\n",
    "        x=values,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        colors=my_colors,\n",
    "        labels=labels,\n",
    "        pctdistance=0.9,\n",
    "        labeldistance=1.05,\n",
    "        startangle=90,\n",
    "        counterclock=False,\n",
    "        textprops={**font_body},\n",
    "    )\n",
    "    ax.axis(\"equal\")\n",
    "    savepdf(fig, \"pie_%s\" % save_fig_name.lower().replace(\" \", \"_\").replace(\")\", \"\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T14:32:11.153609Z",
     "iopub.status.busy": "2021-11-05T14:32:11.153266Z",
     "iopub.status.idle": "2021-11-05T14:32:11.198310Z",
     "shell.execute_reply": "2021-11-05T14:32:11.186784Z",
     "shell.execute_reply.started": "2021-11-05T14:32:11.153568Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Heatmap showing feature correlation (nonparametric)\n",
    "def correlation_matrix(df, figsize=(5, 5), save_fig_name=None):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cmap = matplotlib.cm.get_cmap(\"coolwarm\", 24)\n",
    "    cax = ax.imshow(df.corr(\"spearman\"), cmap=cmap, vmin=-1, vmax=1)\n",
    "    plt.title(\"Feature Correlation\")\n",
    "    labels = [f.split(\"_\")[1] for f in df.columns.tolist()]\n",
    "    ax.set_xticks(range(len(df.columns)))\n",
    "    ax.set_yticks(range(len(df.columns)))\n",
    "    ax.set_xticklabels(labels, fontsize=8, rotation=90)\n",
    "    ax.set_yticklabels(labels, fontsize=8)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    if np.all(save_fig_name != None):\n",
    "        savepdf(fig, \"feature_correlation_%s\" % save_fig_name)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling classes, functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:46.839950Z",
     "start_time": "2020-09-14T00:07:45.675693Z"
    },
    "execution": {
     "iopub.execute_input": "2021-06-09T09:16:12.164493Z",
     "iopub.status.busy": "2021-06-09T09:16:12.163976Z",
     "iopub.status.idle": "2021-06-09T09:16:12.395860Z",
     "shell.execute_reply": "2021-06-09T09:16:12.394817Z",
     "shell.execute_reply.started": "2021-06-09T09:16:12.164409Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "    \"\"\"\n",
    "    Set up grid search across multiple estimators, pipelines; automatically\n",
    "    performs stratified CV if labels are multiclass.\n",
    "\n",
    "    By David Bastista:\n",
    "    http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\n",
    "                \"Some estimators are missing parameters: %s\" % missing_params\n",
    "            )\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=5, n_jobs=1, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(\n",
    "                model,\n",
    "                params,\n",
    "                cv=cv,\n",
    "                n_jobs=n_jobs,\n",
    "                pre_dispatch=2 * n_jobs,\n",
    "                verbose=verbose,\n",
    "                scoring=scoring,\n",
    "                refit=refit,\n",
    "                return_train_score=True,\n",
    "            )\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by=\"mean_score\"):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                \"estimator\": key,\n",
    "                \"min_score\": min(scores),\n",
    "                \"max_score\": max(scores),\n",
    "                \"mean_score\": np.mean(scores),\n",
    "                \"std_score\": np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_[\"params\"]\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values(\n",
    "            [\"mean_score\", \"max_score\"], ascending=[False, False]\n",
    "        )\n",
    "\n",
    "        columns = [\"estimator\", \"min_score\", \"mean_score\", \"max_score\", \"std_score\"]\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T18:09:08.930544Z",
     "start_time": "2020-02-10T18:09:08.918551Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-05T14:36:48.079399Z",
     "iopub.status.busy": "2021-11-05T14:36:48.079022Z",
     "iopub.status.idle": "2021-11-05T14:36:48.177970Z",
     "shell.execute_reply": "2021-11-05T14:36:48.176980Z",
     "shell.execute_reply.started": "2021-11-05T14:36:48.079357Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_conf_matrix(\n",
    "    cm,\n",
    "    classes,\n",
    "    normalize=True,\n",
    "    showxlabel=False,\n",
    "    showylabel=False,\n",
    "    cmap=matplotlib.cm.Blues,\n",
    "):\n",
    "    \"\"\"Print and plot a confusion matrix\n",
    "\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "\n",
    "    Adapted from:\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import itertools\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title = \"Normalized Confusion Matrix\"\n",
    "    else:\n",
    "        title = \"Confusion Matrix\"\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.rc(\"font\", size=12)  # controls default text size\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap, vmin=0, vmax=1)\n",
    "    # plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.gcf().subplots_adjust(bottom=0.2)\n",
    "    if showylabel:\n",
    "        plt.ylabel(\"True weight fraction\")\n",
    "    if showxlabel:\n",
    "        plt.xlabel(\"Predicted weight fraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References for implementing the augmentation functions: \n",
    "* https://stackoverflow.com/questions/34226400/find-the-index-of-the-k-smallest-values-of-a-numpy-array\n",
    "* https://stackoverflow.com/questions/22117834/how-do-i-return-a-list-of-the-3-lowest-values-in-another-list\n",
    "* http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:16:12.837588Z",
     "iopub.status.busy": "2021-06-09T09:16:12.836982Z",
     "iopub.status.idle": "2021-06-09T09:16:12.958065Z",
     "shell.execute_reply": "2021-06-09T09:16:12.957273Z",
     "shell.execute_reply.started": "2021-06-09T09:16:12.837529Z"
    }
   },
   "outputs": [],
   "source": [
    "import random as pyrandom\n",
    "from numpy import random\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define feature mask for data augmentation\n",
    "feat_names = X_enm.columns\n",
    "col_mask = [\"cprp\" not in name for name in feat_names]\n",
    "\n",
    "\n",
    "# Functions for different data augmentation methods\n",
    "\n",
    "\n",
    "def random_augment(k, X_source, y_source, random_state, X, y):\n",
    "    \"\"\"Randomly samples source data to pair with target data.\"\"\"\n",
    "\n",
    "    if k == 0:\n",
    "        return X, y\n",
    "\n",
    "    pyrandom.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Number of samples to select\n",
    "    n_samples = k * len(X)\n",
    "    # Obtain indices for randomly sampling source data\n",
    "    idx_match = np.random.choice(len(X_source), n_samples)\n",
    "    # Select matching rows from source data\n",
    "    X_match = X_source.iloc[idx_match, :]\n",
    "    y_match = y_source[idx_match]\n",
    "    # Append sampled source data to target data\n",
    "    X_aug = np.concatenate((X, X_match))\n",
    "    y_aug = np.concatenate((y, y_match))\n",
    "    assert (\n",
    "        X_aug.shape[0] == y_aug.shape[0]\n",
    "    ), f\"X_aug.shape={X_aug.shape}, y_aug.shape={y_aug.shape}\"\n",
    "\n",
    "    return X_aug, y_aug\n",
    "\n",
    "\n",
    "def unsupervised_augment(k, X_source, y_source, random_state, X, y):\n",
    "    \"\"\"\n",
    "    Unsupervised data augmentation\n",
    "\n",
    "    Match \"k\" most similar source data samples to target data samples\n",
    "    based on the smallest cosine distance between target and source data\n",
    "    samples (i.e., in an supervised fashion).\n",
    "    \"\"\"\n",
    "\n",
    "    if k == 0:\n",
    "        return X, y\n",
    "\n",
    "    pyrandom.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Cosine distance matrix using feature mask\n",
    "    cosdist_samples = cosine_distances(X_source * col_mask, X * col_mask)\n",
    "    # Loop over distance matrix in search of k-smallest distances\n",
    "    idx_match = []\n",
    "    for col in cosdist_samples.T:\n",
    "        # Find organics data indices of k-smallest distances\n",
    "        matches = np.argpartition(col, k)[:k]\n",
    "        idx_match.extend(matches)\n",
    "    # Select matching rows from source data\n",
    "    X_match = X_source.iloc[idx_match, :]\n",
    "    y_match = y_source[idx_match]\n",
    "    # Append sampled source data to target data\n",
    "    X_aug = np.concatenate((X, X_match))\n",
    "    y_aug = np.concatenate((y, y_match))\n",
    "\n",
    "    return X_aug, y_aug\n",
    "\n",
    "\n",
    "def supervised_augment(k, X_source, y_source, random_state, X, y):\n",
    "    \"\"\"\n",
    "    Supervised data augmentation\n",
    "\n",
    "    Match \"k\" most similar source data samples to target data samples\n",
    "    based on the smallest average of cosine distance between samples\n",
    "    and distance between WF labels (i.e., in an supervised fashion).\n",
    "    \"\"\"\n",
    "\n",
    "    if k == 0:\n",
    "        return X, y\n",
    "\n",
    "    pyrandom.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Cosine distance matrix using feature mask\n",
    "    cosdist_samples = cosine_distances(X_source * col_mask, X * col_mask)\n",
    "    # For supervised matching augmentation, also consider WF labels\n",
    "    # Turn 1D label arrays into 2D arrays\n",
    "    y_2d = np.tile(y, (len(y_source), 1))\n",
    "    y_source_2d = np.tile(y_source, (len(y), 1)).transpose()\n",
    "    # Get normalized distance between ENM and organics labels\n",
    "    scaler = MinMaxScaler()\n",
    "    dist_y = scaler.fit_transform(np.abs(y_2d - y_source_2d).astype(float))\n",
    "    # Weighted average distances of features and labels\n",
    "    dist_matrix = (0.95 * cosdist_samples) + (0.05 * dist_y)\n",
    "    # Loop over distance matrix in search of k-smallest distances\n",
    "    idx_match = []\n",
    "    for col in dist_matrix.T:\n",
    "        # Find organics data indices of k-smallest distances\n",
    "        matches = np.argpartition(col, k)[:k]\n",
    "        idx_match.extend(matches)\n",
    "    # Select matching rows from source data\n",
    "    X_match = X_source.iloc[idx_match, :]\n",
    "    y_match = y_source[idx_match]\n",
    "    # Append sampled source data to target data\n",
    "    X_aug = np.concatenate((X, X_match))\n",
    "    y_aug = np.concatenate((y, y_match))\n",
    "\n",
    "    return X_aug, y_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:16:12.994902Z",
     "iopub.status.busy": "2021-06-09T09:16:12.994480Z",
     "iopub.status.idle": "2021-06-09T09:16:13.093350Z",
     "shell.execute_reply": "2021-06-09T09:16:13.092536Z",
     "shell.execute_reply.started": "2021-06-09T09:16:12.994844Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import sys\n",
    "\n",
    "\n",
    "class AugmentingPipeline(Pipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps,\n",
    "        *,\n",
    "        augmentation_type=None,\n",
    "        augmentation_k=None,\n",
    "        augmentation_X_source=None,\n",
    "        augmentation_y_source=None,\n",
    "        augmentation_random_state=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.augmentation_type = augmentation_type\n",
    "        self.augmentation_k = augmentation_k\n",
    "        self.augmentation_X_source = augmentation_X_source\n",
    "        self.augmentation_y_source = augmentation_y_source\n",
    "        self.augmentation_random_state = augmentation_random_state\n",
    "        super().__init__(steps, **kwargs)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = super().get_params(deep=deep)\n",
    "        return {\n",
    "            \"augmentation_type\": self.augmentation_type,\n",
    "            \"augmentation_k\": self.augmentation_k,\n",
    "            \"augmentation_X_source\": self.augmentation_X_source,\n",
    "            \"augmentation_y_source\": self.augmentation_y_source,\n",
    "            \"augmentation_random_state\": self.augmentation_random_state,\n",
    "            **params,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        if \"augmentation_type\" in kwargs:\n",
    "            self.augmentation_type = kwargs[\"augmentation_type\"]\n",
    "        if \"augmentation_k\" in kwargs:\n",
    "            self.augmentation_k = kwargs[\"augmentation_k\"]\n",
    "        if \"augmentation_X_source\" in kwargs:\n",
    "            self.augmentation_X_source = kwargs[\"augmentation_X_source\"]\n",
    "        if \"augmentation_y_source\" in kwargs:\n",
    "            self.augmentation_y_source = kwargs[\"augmentation_y_source\"]\n",
    "        super().set_params(**kwargs)\n",
    "        return self\n",
    "\n",
    "    def _augment(self, X, y):\n",
    "        \"\"\"Apply specified data augmentation function\"\"\"\n",
    "\n",
    "        return self.augmentation_type(\n",
    "            self.augmentation_k,\n",
    "            self.augmentation_X_source,\n",
    "            self.augmentation_y_source,\n",
    "            self.augmentation_random_state,\n",
    "            X,\n",
    "            y,\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # print(\"X dim\", X.shape, file=sys.stderr)\n",
    "        X, y = self._augment(X, y)  # Apply data augmentation\n",
    "        # print(\"X_aug dim\", X.shape, file=sys.stderr)\n",
    "        # import pdb; pdb.set_trace()  # Option to run debugger\n",
    "        return super().fit(X, y, **fit_params)\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        \"\"\"fit_transform but with data augmentation. Only applies to training data.\"\"\"\n",
    "        print(\"fit_transform was called.\", file=sys.stderr)\n",
    "        X, y = self._augment(X, y)  # Apply data augmentation\n",
    "        return super().fit_transform(X, y, **fit_params)\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        print(\"transform was called.\", file=sys.stderr)\n",
    "        return super().transform(X, y, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:16:13.164423Z",
     "iopub.status.busy": "2021-06-09T09:16:13.163832Z",
     "iopub.status.idle": "2021-06-09T09:16:13.313813Z",
     "shell.execute_reply": "2021-06-09T09:16:13.312953Z",
     "shell.execute_reply.started": "2021-06-09T09:16:13.164337Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "\n",
    "def apply_model_opt(\n",
    "    classifiers,\n",
    "    params,\n",
    "    X_target=X_enm,\n",
    "    y_target=bin_enm,\n",
    "    cust_folds=None,\n",
    "    random_state=None,\n",
    "    n_jobs=3,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Optimize classifier parameters.\n",
    "\n",
    "    Returns table of performance results across a grid of parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set smallest class size as number of CV folds for leave-one-out CV\n",
    "    _, class_counts = np.unique(y_target, return_counts=True)\n",
    "    n_folds = min(class_counts)\n",
    "    # Ignore n_folds above if custom CV fold is specified\n",
    "    if cust_folds:\n",
    "        n_folds = cust_folds\n",
    "    else:\n",
    "        n_folds = n_folds\n",
    "\n",
    "    # Apply\n",
    "    helper = EstimatorSelectionHelper(classifiers, params)\n",
    "    helper.fit(\n",
    "        X_target,\n",
    "        y_target,\n",
    "        n_jobs=n_jobs,\n",
    "        cv=n_folds,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "    )\n",
    "    results = helper.score_summary(sort_by=\"mean_score\")\n",
    "    results.columns = [col.split(\"__\")[-1] for col in results.columns]\n",
    "    # results[\"augmentation_type\"] = [str(i).split(\" \")[1] for i in results.loc[:, \"augmentation_type\"]]\n",
    "\n",
    "    return results.infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:16:13.340241Z",
     "iopub.status.busy": "2021-06-09T09:16:13.339870Z",
     "iopub.status.idle": "2021-06-09T09:16:13.453311Z",
     "shell.execute_reply": "2021-06-09T09:16:13.452593Z",
     "shell.execute_reply.started": "2021-06-09T09:16:13.340190Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_eval(\n",
    "    classifier,\n",
    "    featreducer,\n",
    "    augmentation_type,\n",
    "    augmentation_k,\n",
    "    X=X_enm,\n",
    "    y=bin_enm,\n",
    "    X_source=X_source,\n",
    "    y_source=bin_source,\n",
    "    random_state=np.arange(100),\n",
    "    cust_folds=None,\n",
    "    save_fig_name=None,\n",
    "    show_conf_matrix=True,\n",
    "):\n",
    "    \"\"\"Fit execute and evaluate a classifier using hyperparameters.\n",
    "\n",
    "    1) Fit custom model pipeline that performs data augmentation and\n",
    "    normalization on training data using optimized parameters and\n",
    "    stratified k-fold cross validation;\n",
    "    3) Execute optimized model and summarize its accuracy in a confusion\n",
    "    matrix broken down by WF bins. Formatted confusion matrices are saved as\n",
    "    .pdf files.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    classifier : function\n",
    "    featreducer: function\n",
    "    augmentation_type : function\n",
    "    k : int\n",
    "        The number of organics samples to match with each ENM sample.\n",
    "    X : DataFrame (default=X_enm)\n",
    "        Target feature data.\n",
    "    y : ndarray (default=bin_enm)\n",
    "        Target WF bin data\n",
    "    X_source : DataFrame (default=X_enm)\n",
    "        Source feature data.\n",
    "    y_source : ndarray (default=bin_enm)\n",
    "        Source WF bin data.\n",
    "    random_state : ndarray (default=np.arange(100))\n",
    "        Option to set the seed for CV.\n",
    "    save_fig_name : string (default=None)\n",
    "        A unique string used at the end of confusion matrix and feature\n",
    "        importance (rfc-only) file names for exporting the figures as .pdf;\n",
    "        `None` indicates that no figures should be saved\n",
    "    show_cnf_matrix : bool (default=True)\n",
    "        `True` results in matrix graphics being printed as output\n",
    "    \"\"\"\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def standard_error(bal_accu):\n",
    "        \"\"\"Calculate standard error from an array of balanced accuracies.\"\"\"\n",
    "        n = bal_accu.size\n",
    "        samp_var = np.var(bal_accu, ddof=1)\n",
    "        return np.sqrt(samp_var / n)\n",
    "\n",
    "    feat_names = X.columns.values\n",
    "    X = np.array(X)\n",
    "    n_b = len(np.unique(y))  # Check number of bins\n",
    "\n",
    "    # Set smallest class size as number of CV folds for leave-one-out CV\n",
    "    _, class_counts = np.unique(y, return_counts=True)\n",
    "    n_folds = min(class_counts)\n",
    "    # Custom folds override\n",
    "    if cust_folds:\n",
    "        n_folds = cust_folds\n",
    "    else:\n",
    "        n_folds = n_folds\n",
    "\n",
    "    augmentation_kwargs = {\n",
    "        \"augmentation_X_source\": X_source,\n",
    "        \"augmentation_y_source\": y_source,\n",
    "        \"augmentation_type\": augmentation_type,\n",
    "        \"augmentation_k\": augmentation_k,\n",
    "    }\n",
    "\n",
    "    # Placeholders for confusion matrix (cm), feature importance\n",
    "    cm_cum_state = np.zeros([n_b, n_b])\n",
    "    arr_norm_state_avg = []\n",
    "    std_err = []\n",
    "\n",
    "    for state in random_state:\n",
    "        # CV settings\n",
    "        skfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=state)\n",
    "        # Placeholders\n",
    "        arr_cm_norm_fold = np.zeros([n_folds, n_b, n_b])\n",
    "        arr_norm_fold_avg = []\n",
    "\n",
    "        # Fit and run pipeline\n",
    "        for i, (train_index, test_index) in enumerate(skfold.split(X, y)):\n",
    "            # Split data\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "            # Pipeline with data augmentation\n",
    "            pipe = AugmentingPipeline(\n",
    "                [\n",
    "                    (\"scaler\", MinMaxScaler()),\n",
    "                    (\"dimreducer\", featreducer),\n",
    "                    (\"estimator\", classifier.set_params(random_state=state)),\n",
    "                ],\n",
    "                **augmentation_kwargs\n",
    "            )\n",
    "            pipe.fit(X_train, y_train)  # TODO: DOUBLE CHECK\n",
    "\n",
    "            # Write prediction results to confusion matrix\n",
    "            cm_fold = np.zeros([n_b, n_b])\n",
    "            cm_fold = confusion_matrix(y_test, pipe.predict(X_test))  # size 3x3\n",
    "            # Normalize balanced accuracy\n",
    "            arr_cm_norm_fold[i, :, :] = (\n",
    "                cm_fold.astype(\"float\") / cm_fold.sum(axis=1)[:, np.newaxis]\n",
    "            )  # 10x3x3\n",
    "            # Get balanced average proportion of correct classifications\n",
    "            arr_norm_fold_avg.append(\n",
    "                arr_cm_norm_fold[i, :, :].diagonal().mean()\n",
    "            )  # size 10\n",
    "\n",
    "        # Accumulate data for each fold (n_fold total) over multiple trials\n",
    "        cm_cum_state += np.average(arr_cm_norm_fold, axis=0)  # size 3x3\n",
    "        arr_norm_state_avg.append(np.average(arr_norm_fold_avg))  # size 100\n",
    "        std_err.append(standard_error(bal_accu=np.array(arr_norm_fold_avg)))  # size 100\n",
    "\n",
    "    # Average over multiple trials\n",
    "    cm_avg_all = cm_cum_state / len(random_state)  # size 3x3\n",
    "    std_err_avg = np.average(np.array(std_err))\n",
    "\n",
    "    # Average normalized balanced accuracy overall\n",
    "    bal_accu_avg = cm_avg_all.diagonal().mean()\n",
    "\n",
    "    # Plot and save normalized confusion matrix, optional feature importance\n",
    "    fig = plt.figure()\n",
    "    plot_conf_matrix(cm_avg_all, classes=[\"low\", \"mid\", \"high\"])\n",
    "    if np.all(save_fig_name != None):\n",
    "        savepdf(fig, \"confusion_norm_%s\" % save_fig_name)\n",
    "    if not show_conf_matrix:\n",
    "        plt.close(fig)\n",
    "\n",
    "    return arr_norm_state_avg, std_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:16:13.562515Z",
     "iopub.status.busy": "2021-06-09T09:16:13.562047Z",
     "iopub.status.idle": "2021-06-09T09:16:13.676414Z",
     "shell.execute_reply": "2021-06-09T09:16:13.675580Z",
     "shell.execute_reply.started": "2021-06-09T09:16:13.562460Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_hyperparams(\n",
    "    df_params,\n",
    "    random_state=np.arange(100),\n",
    "    X=X_enm,\n",
    "    y=bin_enm,\n",
    "    cust_folds=None,\n",
    "    fig_name_prefix=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply hyperparameters for model evaluation.\n",
    "\n",
    "    Relies on `model_eval' function.\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    # Select for highest performing parameters (averaged across 5 folds);\n",
    "    # note that these are sorted by best mean score\n",
    "    df_params[\"augmentation_type\"] = [\n",
    "        str(i).split(\" \")[1] for i in df_params.loc[:, \"augmentation_type\"]\n",
    "    ]\n",
    "    feat_subset = [\"estimator\", \"augmentation_k\", \"augmentation_type\"]\n",
    "    df_params = df_params.drop_duplicates(subset=feat_subset, keep=\"first\")\n",
    "    df_params = df_params.sort_values(by=feat_subset, ascending=[False, True, True])\n",
    "    df_params.columns = [col.split(\"__\")[-1] for col in df_params.columns]\n",
    "    df_params = df_params.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Placeholder lists for output\n",
    "    df_params[[\"score_avg\", \"score_list\", \"std_err\"]] = np.nan\n",
    "    df_params[[\"score_avg\", \"score_list\", \"std_err\"]] = df_params[\n",
    "        [\"score_avg\", \"score_list\", \"std_err\"]].astype(object)\n",
    "\n",
    "    for row in df_params.index:\n",
    "        # Dictionary of classifier parameters, dropping N/A parameter(s)\n",
    "        # Note: Adjust filter if grid has more/less than 3 classifier parameters\n",
    "        cls_kwargs = df_params.iloc[row, -6:-3].dropna().to_dict()\n",
    "        # Define classifier using parameters dictionary\n",
    "        if df_params.loc[row, \"estimator\"] == \"SVC\":\n",
    "            classifier = SVC(kernel=\"rbf\", class_weight=\"balanced\", **cls_kwargs)\n",
    "        else:\n",
    "            classifier = RandomForestClassifier(class_weight=\"balanced\", **cls_kwargs)\n",
    "        # Define feature reducer\n",
    "        pca = PCA(n_components=df_params.n_components[row], copy=True)\n",
    "        # Make sure augmentation_type is a function\n",
    "        augmentation_type = df_params.augmentation_type[row]\n",
    "        dispatcher = {\n",
    "            \"random_augment\": random_augment,\n",
    "            \"unsupervised_augment\": unsupervised_augment,\n",
    "            \"supervised_augment\": supervised_augment,\n",
    "        }\n",
    "        if isinstance(augmentation_type, str):\n",
    "            augmentation_type = dispatcher[augmentation_type]\n",
    "        # Name figures using concatenated strings\n",
    "        save_fig_name = \"_\".join(map(str, list(df_params.loc[row, feat_subset])))\n",
    "        if fig_name_prefix:\n",
    "            save_fig_name = fig_name_prefix + \"_\" + save_fig_name\n",
    "        else: \n",
    "            save_fig_name = save_fig_name\n",
    "\n",
    "        # Make parameter dict\n",
    "        params = {\n",
    "            \"classifier\": classifier,\n",
    "            \"featreducer\": pca,\n",
    "            \"augmentation_type\": augmentation_type,\n",
    "            \"augmentation_k\": df_params.augmentation_k[row],\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"random_state\": random_state,\n",
    "            \"cust_folds\": cust_folds,\n",
    "            \"save_fig_name\": save_fig_name,\n",
    "        }\n",
    "        score, err = model_eval(**params)\n",
    "        df_params.score_list[row] = score\n",
    "        df_params.std_err[row] = err\n",
    "        df_params.score_avg[row] = np.average(df_params.score_list[row])\n",
    "    cols_report = [\n",
    "        \"estimator\",\n",
    "        \"augmentation_type\",\n",
    "        \"augmentation_k\",\n",
    "        \"score_avg\",\n",
    "        \"score_list\",\n",
    "        \"std_err\",\n",
    "    ]\n",
    "\n",
    "    return df_params[cols_report]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T01:42:56.770018Z",
     "start_time": "2020-09-14T01:42:52.872064Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-05T14:37:01.635469Z",
     "iopub.status.busy": "2021-11-05T14:37:01.635124Z",
     "iopub.status.idle": "2021-11-05T14:37:04.213561Z",
     "shell.execute_reply": "2021-11-05T14:37:04.212517Z",
     "shell.execute_reply.started": "2021-11-05T14:37:01.635433Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook functions.ipynb to script\n",
      "[NbConvertApp] Writing 31716 bytes to functions.py\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    !jupyter nbconvert --to script functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
