{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the source file for classes and functions used throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %load_ext lab_black\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Couldn't load Black autoformatter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:40.730556Z",
     "start_time": "2020-09-14T00:07:39.596248Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.display import display\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:41.251584Z",
     "start_time": "2020-09-14T00:07:41.242136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure directory exists.\n"
     ]
    }
   ],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "PROJECT_SAVE_DIR = \"figs\"\n",
    "\n",
    "if not (os.path.isdir(PROJECT_ROOT_DIR+'/'+PROJECT_SAVE_DIR)):\n",
    "    print('Figure directory did not exist, creating now.')\n",
    "    os.mkdir(PROJECT_ROOT_DIR+'/'+PROJECT_SAVE_DIR)\n",
    "else:\n",
    "    print('Figure directory exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:42.188636Z",
     "start_time": "2020-09-14T00:07:42.015127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in target (ENM) model feature data\n",
    "X_enm = pd.read_csv(\"./data/ENM-preprocessed-feats.csv\", \n",
    "                    sep='\\t', header='infer', index_col=0)\n",
    "\n",
    "# Read in source (organics) model feature data\n",
    "X_source = pd.read_csv(\"./data/organics-preprocessed-feats.csv\", \n",
    "                       sep='\\t', header='infer', index_col=0)\n",
    "\n",
    "# Read in ENM labels (maximum_weight_fraction)\n",
    "y_enm = pd.read_csv(\"./data/ENM-clean.csv\", \n",
    "                    sep=',', header='infer', usecols=[4])\n",
    "\n",
    "# Read in organics labels (maximum_weight_fraction)\n",
    "y_source = pd.read_csv(\"./data/organics-preprocessed-WF.csv\", \n",
    "                       sep='\\t', header='infer', index_col=0)\n",
    "y_source.index = X_source.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility classes, functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:48.397494Z",
     "start_time": "2020-09-14T00:07:48.384565Z"
    }
   },
   "outputs": [],
   "source": [
    "def savepdf(fig, name):\n",
    "    \"\"\"Save figures as .pdf files\"\"\"\n",
    "\n",
    "    # Adjust matplotlib settings so text is editable in PDFs\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    plt.rcParams[\"font.sans-serif\"] = \"Helvetica\"\n",
    "    plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "    plt.rcParams[\"ps.fonttype\"] = 42\n",
    "    # Save PDF in figure directory\n",
    "    fig.savefig(\n",
    "        PROJECT_ROOT_DIR + \"/\" + PROJECT_SAVE_DIR + \"/\" + name + \".pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        transparent=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:44.171123Z",
     "start_time": "2020-09-14T00:07:44.161208Z"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    \"\"\"\n",
    "    Option to suppress print output.\n",
    "    \n",
    "    Source:\n",
    "    https://stackoverflow.com/questions/8391411/suppress-calls-to-print-python\n",
    "    \"\"\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:44.837323Z",
     "start_time": "2020-09-14T00:07:44.819475Z"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "class CompletionNotifier:\n",
    "    \"\"\"Nestable function-completion notification decorator\n",
    "    \n",
    "    I.e., no notification sound on inner functions.\n",
    "    \n",
    "    Source:\n",
    "    https://gist.github.com/jdpage/26376472ac18a7057e381f2259c7b988\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, notify_fun):\n",
    "        self.notify_fun = notify_fun\n",
    "        self.isinner = False\n",
    "\n",
    "    def __call__(self, f):\n",
    "        # return f # to disable\n",
    "        @functools.wraps(f)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            isinner = self.isinner\n",
    "            self.isinner = True\n",
    "            try:\n",
    "                return f(*args, **kwargs)\n",
    "            finally:\n",
    "                self.isinner = isinner\n",
    "                if not self.isinner:\n",
    "                    self.notify_fun(f)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "\n",
    "# To enable notification sound to play\n",
    "def ping_notify(*args):\n",
    "    from IPython.display import Audio\n",
    "\n",
    "    sound_file = \"./data/ping.wav\"\n",
    "    display(Audio(url=sound_file, autoplay=True))\n",
    "\n",
    "\n",
    "notify_on_complete = CompletionNotifier(ping_notify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bins(row):\n",
    "    \"\"\"Assign weight fractions (continuous) to bins (int)\n",
    "    \n",
    "    Class ranges are different from those used by Isaacs et al. 2016.\n",
    "    \"\"\"\n",
    "\n",
    "    if row[\"maximum_weight_fraction\"] <= 0.01:\n",
    "        val = 0  # low\n",
    "    elif row[\"maximum_weight_fraction\"] > 0.10:\n",
    "        val = 2  # high\n",
    "    else:\n",
    "        val = 1  # medium\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:30:02.159055Z",
     "start_time": "2020-09-14T00:30:02.152808Z"
    }
   },
   "outputs": [],
   "source": [
    "# def bins(row):\n",
    "#    \"\"\"Assign weight fractions (continuous) to bins (int)\n",
    "#\n",
    "#    Class ranges are different from those used by Isaacs et al. 2016.\n",
    "#    \"\"\"\n",
    "#\n",
    "#    if row['maximum_weight_fraction'] < 0.0001:\n",
    "#        val = 0 # xlow\n",
    "#    elif row['maximum_weight_fraction'] >= 0.2:\n",
    "#        val = 3 # high\n",
    "#    elif row['maximum_weight_fraction'] >= 0.01:\n",
    "#        val = 2 # medium\n",
    "#    else:\n",
    "#        val = 1 # low\n",
    "#    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bins(row):\n",
    "#    \"\"\"Assign weight fractions (continuous) to bins (int)\n",
    "#\n",
    "#    Class ranges are different from those used by Isaacs et al. 2016.\n",
    "#    \"\"\"\n",
    "#\n",
    "#    if row['maximum_weight_fraction'] <= 0.0001:\n",
    "#        val = 0 # low\n",
    "#    elif row['maximum_weight_fraction'] > 0.01:\n",
    "#        val = 2 # high\n",
    "#    else:\n",
    "#        val = 1 # medium\n",
    "#    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:08:00.905642Z",
     "start_time": "2020-09-14T00:08:00.345259Z"
    }
   },
   "outputs": [],
   "source": [
    "bin_enm = np.asarray(y_enm.apply(bins, axis=1))\n",
    "bin_source = np.asarray(y_source.apply(bins, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:08:01.617402Z",
     "start_time": "2020-09-14T00:08:01.601209Z"
    }
   },
   "outputs": [],
   "source": [
    "def bar_graph_bins(label_data, data_composition):\n",
    "    \"\"\"Bar graph of weight fraction bins\n",
    "    \n",
    "    Create a bar graph of weight fraction bins and print the \n",
    "    count and frequency for each.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    label_data : int array of shape [n,]\n",
    "        Dataframe containing binned wf data\n",
    "    data_composition : string\n",
    "        Describes the chemical composition of label_data \n",
    "        for use in the plot title; e.g., `ENM`, `Organics`   \n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Find the count, frequency of WF bins\n",
    "    unique, counts = np.unique(label_data, return_counts=True)\n",
    "    wf_distrib = dict(zip(unique, counts))\n",
    "    freq = []\n",
    "    for i in counts:\n",
    "        percent = (i / np.sum(counts)).round(2)\n",
    "        freq.append(percent)\n",
    "    bin_names = [\"low\", \"medium\", \"high\"]\n",
    "    if len(unique) == 4:\n",
    "        bin_names = [\"xlow\"] + bin_names\n",
    "    # Plot\n",
    "    plt.bar(range(len(wf_distrib)), list(wf_distrib.values()), align=\"center\")\n",
    "    plt.xticks(range(len(wf_distrib)), list(bin_names))\n",
    "    plt.title(\"Frequency of %s Weight Fraction Bins\" % data_composition)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Label bin: \", unique)\n",
    "    print(\"Count    : \", counts)\n",
    "    print(\"Frequency: \", freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro-Wilk Test for normality\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "\n",
    "def norm_test(data):\n",
    "    \"\"\"\n",
    "    Source:\n",
    "    https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/\n",
    "    \"\"\"\n",
    "    # Normality test\n",
    "    stat, p = shapiro(data)\n",
    "    print(\"Statistics=%.3f, p=%.3f\" % (stat, p))\n",
    "    # Interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print(\"Sample looks Gaussian (fail to reject H0)\")\n",
    "    else:\n",
    "        print(\"Sample does not look Gaussian (reject H0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T18:09:07.022349Z",
     "start_time": "2020-02-10T18:09:07.010399Z"
    }
   },
   "outputs": [],
   "source": [
    "def feat_agglom(n_clust, prefix, df_fit, df_trans=None):\n",
    "    \"\"\"Apply feature agglomeration to get a list of new column names\n",
    "    \n",
    "    If `df_trans` is provided, returns the reduced feature data frame.\n",
    "    `prefix` is the four character indicator that the column was agglomerated.\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "    # ===== Fit feature agglomeration =====\n",
    "    agg = FeatureAgglomeration(n_clust, affinity=\"cosine\", linkage=\"average\")\n",
    "    agg.fit(df_fit + 0.0001)  # Add small fraction so features don't disappear\n",
    "\n",
    "    # ===== Get agglomerated feature labels =====\n",
    "    # Create array showing order for agglomerated features\n",
    "    ord_arr = np.column_stack((agg.labels_, df_fit.columns))\n",
    "    # Agglomerate names of agglomerated features\n",
    "    len_orig = len(df_fit.columns)\n",
    "    kids = agg.children_\n",
    "    for i in np.arange(0, len_orig - n_clust):\n",
    "        str1, str2 = df_fit.columns[kids[i, 0]], df_fit.columns[kids[i, 1]]\n",
    "        name = \"_\".join([prefix, str1[5:], str2[5:]])\n",
    "        ord_arr[ord_arr == str1] = name\n",
    "        ord_arr[ord_arr == str2] = name\n",
    "    # Sort order\n",
    "    ord_arr = ord_arr[ord_arr[:, 0].argsort()]\n",
    "    # Remove duplicates\n",
    "    agg_cols = list(pd.unique(ord_arr[:, 1]))\n",
    "\n",
    "    # ===== Apply agglomeration =====\n",
    "    if df_trans is not None:\n",
    "        df_red = pd.DataFrame(agg.transform(df_trans), columns=agg_cols)\n",
    "        # Alphabetize features\n",
    "        agg_cols.sort()\n",
    "        df_red = df_red[agg_cols]\n",
    "        return df_red\n",
    "    else:\n",
    "        return agg_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting piecharts\n",
    "def plot_piechart(\n",
    "    data, feat_subset_prefix, save_fig_name, figsize=[3, 2.5], labels=None\n",
    "):\n",
    "\n",
    "    # Aesthetic settings\n",
    "    font_body = {\"fontsize\": 7, \"fontname\": \"Helvetica\"}\n",
    "    my_colors = [\n",
    "        \"tab:blue\",  # sky blue\n",
    "        \"#E59400\",  # orange\n",
    "        \"tab:purple\",  # purple-blue\n",
    "        \"#C0504D\",  # red\n",
    "        \"tab:olive\",\n",
    "        \"#323299\",  # dark blue\n",
    "        \"#7a307a\",  # violet\n",
    "        \"seagreen\",\n",
    "        \"sienna\",\n",
    "        \"gold\",\n",
    "    ]\n",
    "\n",
    "    # Define data and labels for plotting\n",
    "    feat_subset = [f for f in data.columns if feat_subset_prefix in f]\n",
    "    if len(feat_subset) == 1:\n",
    "        pos_labels = np.count_nonzero(data[feat_subset])\n",
    "        values = [(pos_labels), (len(data[feat_subset]) - pos_labels)]\n",
    "        if labels is None:\n",
    "            labels = [feat_subset[0].split(\"_\")[1], \"other\"]\n",
    "        else:\n",
    "            labels = labels\n",
    "\n",
    "    else:\n",
    "        values = data[feat_subset].sum(axis=0)\n",
    "        labels = [f.split(\"_\")[1] for f in feat_subset]\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.pie(\n",
    "        x=values,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        colors=my_colors,\n",
    "        labels=labels,\n",
    "        pctdistance=0.9,\n",
    "        labeldistance=1.05,\n",
    "        startangle=90,\n",
    "        counterclock=False,\n",
    "        textprops={**font_body},\n",
    "    )\n",
    "    ax.axis(\"equal\")\n",
    "    savepdf(fig, \"pie_%s\" % save_fig_name.lower().replace(\" \", \"_\").replace(\")\", \"\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap showing feature correlation (nonparametric)\n",
    "def correlation_matrix(df, figsize=(6, 6), save_fig_name=None):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cmap = matplotlib.cm.get_cmap(\"coolwarm\", 24)\n",
    "    cax = ax.imshow(df.corr(\"spearman\"), cmap=cmap, vmin=-1, vmax=1)\n",
    "    plt.title(\"Feature Correlation\")\n",
    "    labels = [f.split(\"_\")[1] for f in df.columns.tolist()]\n",
    "    ax.set_xticks(range(len(df.columns)))\n",
    "    ax.set_yticks(range(len(df.columns)))\n",
    "    ax.set_xticklabels(labels, fontsize=8, rotation=90)\n",
    "    ax.set_yticklabels(labels, fontsize=8)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    if np.all(save_fig_name != None):\n",
    "        savepdf(fig, \"feature_correlation_%s\" % save_fig_name)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling classes, functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T00:07:46.839950Z",
     "start_time": "2020-09-14T00:07:45.675693Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "    \"\"\"\n",
    "    Set up grid search across multiple estimators, pipelines; automatically \n",
    "    performs stratified CV if labels are multiclass.\n",
    "    \n",
    "    By David Bastista:\n",
    "    http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\n",
    "                \"Some estimators are missing parameters: %s\" % missing_params\n",
    "            )\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=5, n_jobs=1, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(\n",
    "                model,\n",
    "                params,\n",
    "                cv=cv,\n",
    "                n_jobs=n_jobs,\n",
    "                verbose=verbose,\n",
    "                scoring=scoring,\n",
    "                refit=refit,\n",
    "                return_train_score=True,\n",
    "            )\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by=\"mean_score\"):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                \"estimator\": key,\n",
    "                \"min_score\": min(scores),\n",
    "                \"max_score\": max(scores),\n",
    "                \"mean_score\": np.mean(scores),\n",
    "                \"std_score\": np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_[\"params\"]\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values(\n",
    "            [\"mean_score\", \"max_score\"], ascending=[False, False]\n",
    "        )\n",
    "\n",
    "        columns = [\"estimator\", \"min_score\", \"mean_score\", \"max_score\", \"std_score\"]\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T19:02:56.026137Z",
     "start_time": "2020-02-10T19:02:56.004580Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feat_impt(\n",
    "    feat_names, importances, variances=None, save_fig_name=None, combo_impt=False\n",
    "):\n",
    "    \"\"\"Plot bar graph of feature importance\n",
    "    \n",
    "    This function uses results from an rfc as input to plot feature \n",
    "    importance. Here, the rfc determines importance using gini importance, \n",
    "    aka mean decrease impurity. Includes option to combine features into \n",
    "    more easily interpretable groups.\n",
    "    \n",
    "    References:\n",
    "    https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined\n",
    "    https://matplotlib.org/examples/api/barchart_demo.html\n",
    "    https://stackoverflow.com/questions/28931224/adding-value-labels-on-a-matplotlib-bar-chart\n",
    "    https://stackoverflow.com/questions/14849293/python-find-index-position-in-list-based-of-partial-string\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # (Optional) Sum importance by feature group\n",
    "    if combo_impt:\n",
    "        idx_cprp = [i for i, s in enumerate(feat_names) if \"cprp\" in s]\n",
    "        idx_func = [i for i, s in enumerate(feat_names) if \"fagg\" in s]\n",
    "        idx_func += [i for i, s in enumerate(feat_names) if \"func\" in s]\n",
    "        idx_prod = [i for i, s in enumerate(feat_names) if \"pagg\" in s]\n",
    "        idx_prod += [i for i, s in enumerate(feat_names) if \"pgen\" in s]\n",
    "        idx_prod += [i for i, s in enumerate(feat_names) if \"pgrp\" in s]\n",
    "        idx_WFms = [i for i, s in enumerate(feat_names) if \"WFmeas\" in s]\n",
    "        idx_mtrx = [i for i, s in enumerate(feat_names) if \"mtrx\" in s]\n",
    "        idx_mtrx.sort()\n",
    "        idx_mtrx = idx_mtrx[:2]\n",
    "        importances = np.asarray(\n",
    "            [\n",
    "                np.sum(importances[idx_cprp]),\n",
    "                np.sum(importances[idx_func]),\n",
    "                np.sum(importances[idx_prod]),\n",
    "                np.sum(importances[idx_mtrx]),\n",
    "                np.sum(importances[idx_WFms]),\n",
    "            ]\n",
    "        )\n",
    "        # (Optional) Sum variance by feature group\n",
    "        if np.all(variances != None):\n",
    "            variances = np.asarray(\n",
    "                [\n",
    "                    np.sum(variances[idx_cprp]),\n",
    "                    np.sum(variances[idx_func]),\n",
    "                    np.sum(variances[idx_prod]),\n",
    "                    np.sum(variances[idx_mtrx]),\n",
    "                    np.sum(variances[idx_WFms]),\n",
    "                ]\n",
    "            )\n",
    "        feat_names = [\n",
    "            \"chemProperties\",\n",
    "            \"functionalUses\",\n",
    "            \"productCategories\",\n",
    "            \"productMatrix\",\n",
    "            \"WFmeasured\",\n",
    "        ]\n",
    "\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    # (Optional) Add error bars\n",
    "    if np.all(variances != None):\n",
    "        err_bars = np.sqrt(variances)  # standard deviation\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.grid(True)\n",
    "        ax.barh(\n",
    "            range(len(indices)),\n",
    "            importances[indices],\n",
    "            xerr=err_bars[indices],\n",
    "            capsize=3,\n",
    "            align=\"center\",\n",
    "        )\n",
    "    else:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.barh(range(len(indices)), importances[indices], align=\"center\")\n",
    "\n",
    "    # Add grid lines\n",
    "    plt.grid(False)\n",
    "    xax_max = np.round_(np.amax(importances), decimals=1)\n",
    "    ax.set_xticks(np.arange(0, xax_max + 0.05, xax_max / 5))\n",
    "    ax.xaxis.grid(color=\"silver\")\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    # Label parts of plot\n",
    "    ax.set_title(\"Feature Importance\")\n",
    "    ax.set_xlabel(\"Relative Importance\")\n",
    "    ax.set_yticks(np.arange(len(feat_names)))\n",
    "    ax.set_yticklabels([feat_names[i] for i in indices])\n",
    "    # Add importance value labels at the end of bars\n",
    "    if variances is None:\n",
    "        for rect in ax.patches:\n",
    "            # Get X and Y placement of label from rect\n",
    "            x_value = rect.get_width()\n",
    "            y_value = rect.get_y() + rect.get_height() / 2\n",
    "            # Use X value as label and format number with one decimal place\n",
    "            label = \"{:.2f}\".format(x_value)\n",
    "            # Create annotation\n",
    "            plt.annotate(\n",
    "                label,\n",
    "                (x_value, y_value),  # Place label at end of the bar\n",
    "                xytext=(4, 0),  # Horizontally shift label\n",
    "                textcoords=\"offset points\",  # Interpret `xytext` as offset\n",
    "                va=\"center\",\n",
    "                ha=\"left\",\n",
    "            )\n",
    "\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    if combo_impt:\n",
    "        fig.set_size_inches(10, 6)\n",
    "    else:\n",
    "        fig.set_size_inches(10, 10)\n",
    "    if np.all(save_fig_name != None):\n",
    "        savepdf(fig, \"feature_importance_%s\" % save_fig_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T18:09:08.930544Z",
     "start_time": "2020-02-10T18:09:08.918551Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_conf_matrix(cm, classes, normalize=True, cmap=matplotlib.cm.Blues):\n",
    "    \"\"\"Print and plot a confusion matrix\n",
    "    \n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    Adapted from:\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import itertools\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title = \"Normalized Confusion Matrix\"\n",
    "    else:\n",
    "        title = \"Confusion Matrix\"\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.gcf().subplots_adjust(bottom=0.2)\n",
    "    plt.ylabel(\"True weight fraction\")\n",
    "    plt.xlabel(\"Predicted weight fraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References for implementing the augmentation functions: \n",
    "* https://stackoverflow.com/questions/34226400/find-the-index-of-the-k-smallest-values-of-a-numpy-array\n",
    "* https://stackoverflow.com/questions/22117834/how-do-i-return-a-list-of-the-3-lowest-values-in-another-list\n",
    "* http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as pyrandom\n",
    "from numpy import random\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TODO: These variables might cause problems if they're separate\n",
    "# Define feature mask for data augmentation\n",
    "feat_names = X_enm.columns\n",
    "col_mask = [\"cprp\" not in name for name in feat_names]\n",
    "\n",
    "\n",
    "# Functions for different data augmentation methods\n",
    "\n",
    "\n",
    "def random_augment(k, X_source, y_source, random_state, X, y):\n",
    "    \"\"\"Randomly samples source data to pair with target data.\"\"\"\n",
    "\n",
    "    if k == 0:\n",
    "        return X, y\n",
    "\n",
    "    pyrandom.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Number of samples to select\n",
    "    n_samples = k * len(X)\n",
    "    # Obtain indices for randomly sampling source data\n",
    "    idx_match = np.random.choice(len(X_source), n_samples)\n",
    "    # Select matching rows from source data\n",
    "    X_match = X_source.iloc[idx_match, :]\n",
    "    y_match = y_source[idx_match]\n",
    "    # Append sampled source data to target data\n",
    "    X_aug = np.concatenate((X, X_match))\n",
    "    y_aug = np.concatenate((y, y_match))\n",
    "    assert (\n",
    "        X_aug.shape[0] == y_aug.shape[0]\n",
    "    ), f\"X_aug.shape={X_aug.shape}, y_aug.shape={y_aug.shape}\"\n",
    "\n",
    "    return X_aug, y_aug\n",
    "\n",
    "\n",
    "def unsupervised_augment(k, X_source, y_source, random_state, X, y):\n",
    "    \"\"\"\n",
    "    Unsupervised data augmentation\n",
    "    \n",
    "    Match \"k\" most similar source data samples to target data samples \n",
    "    based on the smallest cosine distance between target and source data \n",
    "    samples (i.e., in an supervised fashion).\n",
    "    \"\"\"\n",
    "\n",
    "    if k == 0:\n",
    "        return X, y\n",
    "\n",
    "    pyrandom.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Cosine distance matrix using feature mask\n",
    "    cosdist_samples = cosine_distances(X_source * col_mask, X * col_mask)\n",
    "    # Loop over distance matrix in search of k-smallest distances\n",
    "    idx_match = []\n",
    "    for col in cosdist_samples.T:\n",
    "        # Find organics data indices of k-smallest distances\n",
    "        matches = np.argpartition(col, k)[:k]\n",
    "        idx_match.extend(matches)\n",
    "    # Select matching rows from source data\n",
    "    X_match = X_source.iloc[idx_match, :]\n",
    "    y_match = y_source[idx_match]\n",
    "    # Append sampled source data to target data\n",
    "    X_aug = np.concatenate((X, X_match))\n",
    "    y_aug = np.concatenate((y, y_match))\n",
    "\n",
    "    return X_aug, y_aug\n",
    "\n",
    "\n",
    "def supervised_augment(k, X_source, y_source, random_state, X, y):\n",
    "    \"\"\"\n",
    "    Supervised data augmentation\n",
    "    \n",
    "    Match \"k\" most similar source data samples to target data samples \n",
    "    based on the smallest average of cosine distance between samples \n",
    "    and distance between WF labels (i.e., in an supervised fashion).\n",
    "    \"\"\"\n",
    "\n",
    "    if k == 0:\n",
    "        return X, y\n",
    "\n",
    "    pyrandom.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Cosine distance matrix using feature mask\n",
    "    cosdist_samples = cosine_distances(X_source * col_mask, X * col_mask)\n",
    "    # For supervised matching augmentation, also consider WF labels\n",
    "    # Turn 1D label arrays into 2D arrays\n",
    "    y_2d = np.tile(y, (len(y_source), 1))\n",
    "    y_source_2d = np.tile(y_source, (len(y), 1)).transpose()\n",
    "    # Get normalized distance between ENM and organics labels\n",
    "    scaler = MinMaxScaler()\n",
    "    dist_y = scaler.fit_transform(np.abs(y_2d - y_source_2d).astype(float))\n",
    "    # Weighted average distances of features and labels\n",
    "    dist_matrix = (0.95 * cosdist_samples) + (0.05 * dist_y)\n",
    "    # Loop over distance matrix in search of k-smallest distances\n",
    "    idx_match = []\n",
    "    for col in dist_matrix.T:\n",
    "        # Find organics data indices of k-smallest distances\n",
    "        matches = np.argpartition(col, k)[:k]\n",
    "        idx_match.extend(matches)\n",
    "    # Select matching rows from source data\n",
    "    X_match = X_source.iloc[idx_match, :]\n",
    "    y_match = y_source[idx_match]\n",
    "    # Append sampled source data to target data\n",
    "    X_aug = np.concatenate((X, X_match))\n",
    "    y_aug = np.concatenate((y, y_match))\n",
    "\n",
    "    return X_aug, y_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import sys\n",
    "\n",
    "\n",
    "class AugmentingPipeline(Pipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps,\n",
    "        *,\n",
    "        augmentation_type=None,\n",
    "        augmentation_k=None,\n",
    "        augmentation_X_source=None,\n",
    "        augmentation_y_source=None,\n",
    "        augmentation_random_state=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.augmentation_type = augmentation_type\n",
    "        self.augmentation_k = augmentation_k\n",
    "        self.augmentation_X_source = augmentation_X_source\n",
    "        self.augmentation_y_source = augmentation_y_source\n",
    "        self.augmentation_random_state = augmentation_random_state\n",
    "        super().__init__(steps, **kwargs)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = super().get_params(deep=deep)\n",
    "        return {\n",
    "            \"augmentation_type\": self.augmentation_type,\n",
    "            \"augmentation_k\": self.augmentation_k,\n",
    "            \"augmentation_X_source\": self.augmentation_X_source,\n",
    "            \"augmentation_y_source\": self.augmentation_y_source,\n",
    "            \"augmentation_random_state\": self.augmentation_random_state,\n",
    "            **params,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        if \"augmentation_type\" in kwargs:\n",
    "            self.augmentation_type = kwargs[\"augmentation_type\"]\n",
    "        if \"augmentation_k\" in kwargs:\n",
    "            self.augmentation_k = kwargs[\"augmentation_k\"]\n",
    "        if \"augmentation_X_source\" in kwargs:\n",
    "            self.augmentation_X_source = kwargs[\"augmentation_X_source\"]\n",
    "        if \"augmentation_y_source\" in kwargs:\n",
    "            self.augmentation_y_source = kwargs[\"augmentation_y_source\"]\n",
    "        super().set_params(**kwargs)\n",
    "        return self\n",
    "\n",
    "    def _augment(self, X, y):\n",
    "        \"\"\"Apply specified data augmentation function\"\"\"\n",
    "\n",
    "        return self.augmentation_type(\n",
    "            self.augmentation_k,\n",
    "            self.augmentation_X_source,\n",
    "            self.augmentation_y_source,\n",
    "            self.augmentation_random_state,\n",
    "            X,\n",
    "            y,\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # print(\"X dim\", X.shape, file=sys.stderr)\n",
    "        X, y = self._augment(X, y)  # Apply data augmentation\n",
    "        # print(\"X_aug dim\", X.shape, file=sys.stderr)\n",
    "        # import pdb; pdb.set_trace()  # Option to run debugger\n",
    "        return super().fit(X, y, **fit_params)\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        \"\"\"fit_transform but with data augmentation. Only applies to training data.\"\"\"\n",
    "        print(\"fit_transform was called.\", file=sys.stderr)\n",
    "        X, y = self._augment(X, y)  # Apply data augmentation\n",
    "        return super().fit_transform(X, y, **fit_params)\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        print(\"transform was called.\", file=sys.stderr)\n",
    "        return super().transform(X, y, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "\n",
    "def apply_model_opt(\n",
    "    classifiers,\n",
    "    params,\n",
    "    X_target=X_enm,\n",
    "    y_target=bin_enm,\n",
    "    cust_folds=None,\n",
    "    random_state=None,\n",
    "    n_jobs=3,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Optimize classifier parameters.\n",
    "    \n",
    "    Returns table of performance results across a grid of parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set smallest class size as number of CV folds for leave-one-out CV\n",
    "    _, class_counts = np.unique(y_target, return_counts=True)\n",
    "    n_folds = min(class_counts)\n",
    "    # Ignore n_folds above if custom CV fold is specified\n",
    "    if cust_folds:\n",
    "        n_folds = cust_folds\n",
    "    else:\n",
    "        n_folds = n_folds\n",
    "\n",
    "    # Apply\n",
    "    helper = EstimatorSelectionHelper(classifiers, params)\n",
    "    helper.fit(\n",
    "        X_target, y_target, n_jobs=n_jobs, cv=n_folds, scoring=\"balanced_accuracy\",\n",
    "    )\n",
    "    results = helper.score_summary(sort_by=\"mean_score\")\n",
    "    results.columns = [col.split(\"__\")[-1] for col in results.columns]\n",
    "    # results[\"augmentation_type\"] = [str(i).split(\" \")[1] for i in results.loc[:, \"augmentation_type\"]]\n",
    "\n",
    "    return results.infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @notify_on_complete # play sound at completion\n",
    "def model_eval(\n",
    "    classifier,\n",
    "    augmentation_type,\n",
    "    augmentation_k,\n",
    "    X=X_enm,\n",
    "    y=bin_enm,\n",
    "    X_source=X_source,\n",
    "    y_source=bin_source,\n",
    "    random_state=np.arange(30),\n",
    "    cust_folds=None,\n",
    "    save_fig_name=None,\n",
    "    show_feat_impt=True,\n",
    "    show_conf_matrix=True,\n",
    "):\n",
    "    \"\"\"Fit execute and evaluate a classifier using hyperparameters.\n",
    "    \n",
    "    1) Fit custom model pipeline that performs data augmentation and \n",
    "    normalization on training data using optimized parameters and \n",
    "    stratified k-fold cross validation;\n",
    "    3) Execute optimized model and summarize its accuracy in a confusion \n",
    "    matrix broken down by WF bins. Formatted confusion matrices are saved as \n",
    "    .pdf files.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    classifier : function\n",
    "    augmentation_type : function\n",
    "    k : int\n",
    "        The number of organics samples to match with each ENM sample.\n",
    "    X : DataFrame (default=X_enm)\n",
    "        Target feature data.\n",
    "    y : ndarray (default=bin_enm)\n",
    "        Target WF bin data\n",
    "    X_source : DataFrame (default=X_enm)\n",
    "        Source feature data.\n",
    "    y_source : ndarray (default=bin_enm)\n",
    "        Source WF bin data.\n",
    "    random_state : ndarray (default=np.arange(30))\n",
    "        Option to set the seed for CV.\n",
    "    save_fig_name : string (default=None)\n",
    "        A unique string used at the end of confusion matrix and feature \n",
    "        importance (rfc-only) file names for exporting the figures as .pdf; \n",
    "        `None` indicates that no figures should be saved\n",
    "    show_feat_impt : bool (default=False)\n",
    "        Only applicable when classifier is 'rfc'; `True` takes results from \n",
    "        an rfc as input to plot a bar graph of feature importance.\n",
    "    show_cnf_matrix : bool (default=False)\n",
    "        `True` results in matrix graphics being printed as output\n",
    "    \"\"\"\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def standard_error(bal_accu):\n",
    "        \"\"\"Calculate standard error from an array of balanced accuracies.\"\"\"\n",
    "        n = bal_accu.size\n",
    "        samp_var = np.var(bal_accu, ddof=1)\n",
    "        return np.sqrt(samp_var / n)\n",
    "\n",
    "    feat_names = X.columns.values\n",
    "    X = np.array(X)\n",
    "    n_b = len(np.unique(y))  # Check number of bins\n",
    "\n",
    "    # Set smallest class size as number of CV folds for leave-one-out CV\n",
    "    _, class_counts = np.unique(y, return_counts=True)\n",
    "    n_folds = min(class_counts)\n",
    "    # Custom folds override\n",
    "    if cust_folds:\n",
    "        n_folds = cust_folds\n",
    "    else:\n",
    "        n_folds = n_folds\n",
    "\n",
    "    augmentation_kwargs = {\n",
    "        \"augmentation_X_source\": X_source,\n",
    "        \"augmentation_y_source\": y_source,\n",
    "        \"augmentation_type\": augmentation_type,\n",
    "        \"augmentation_k\": augmentation_k,\n",
    "    }\n",
    "\n",
    "    # Placeholders for confusion matrix (cm), feature importance\n",
    "    cm_cum_state = np.zeros([n_b, n_b])\n",
    "    arr_norm_state_avg = []\n",
    "    std_err = []\n",
    "    impt_cum = np.zeros(len(feat_names))\n",
    "\n",
    "    for state in random_state:\n",
    "        # CV settings\n",
    "        skfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=state)\n",
    "        # Placeholders\n",
    "        arr_cm_norm_fold = np.zeros([n_folds, n_b, n_b])\n",
    "        arr_norm_fold_avg = []\n",
    "        impt_state = np.zeros(len(feat_names))\n",
    "\n",
    "        # Fit and run pipeline\n",
    "        for i, (train_index, test_index) in enumerate(skfold.split(X, y)):\n",
    "            # Split data\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "            # Pipeline with data augmentation\n",
    "            pipe = AugmentingPipeline(\n",
    "                [\n",
    "                    (\"scale\", MinMaxScaler()),\n",
    "                    (\"estimator\", classifier.set_params(random_state=state)),\n",
    "                ],\n",
    "                **augmentation_kwargs\n",
    "            )\n",
    "            pipe.fit(X_train, y_train)\n",
    "\n",
    "            # Optional calculate feature importance (RFC only)\n",
    "            if show_feat_impt:\n",
    "                importances = classifier.feature_importances_\n",
    "                impt_state += importances\n",
    "\n",
    "            # Write prediction results to confusion matrix\n",
    "            cm_fold = np.zeros([n_b, n_b])\n",
    "            cm_fold = confusion_matrix(y_test, pipe.predict(X_test))  # size 3x3\n",
    "            # Normalize balanced accuracy\n",
    "            arr_cm_norm_fold[i, :, :] = (\n",
    "                cm_fold.astype(\"float\") / cm_fold.sum(axis=1)[:, np.newaxis]\n",
    "            )  # 10x3x3\n",
    "            # Get balanced average proportion of correct classifications\n",
    "            arr_norm_fold_avg.append(\n",
    "                arr_cm_norm_fold[i, :, :].diagonal().mean()\n",
    "            )  # size 10\n",
    "\n",
    "        # Accumulate data for each fold (n_fold total) over 30 trials\n",
    "        cm_cum_state += np.average(arr_cm_norm_fold, axis=0)  # size 3x3\n",
    "        arr_norm_state_avg.append(np.average(arr_norm_fold_avg))  # size 30\n",
    "        std_err.append(standard_error(bal_accu=np.array(arr_norm_fold_avg)))  # size 30\n",
    "        if show_feat_impt:\n",
    "            impt_cum += impt_state\n",
    "\n",
    "    # Average over 30 trials\n",
    "    cm_avg_all = cm_cum_state / len(random_state)  # size 3x3\n",
    "    std_err_avg = np.average(np.array(std_err))\n",
    "    impt_avg = np.asarray(\n",
    "        [i / (len(random_state) * n_folds) for i in impt_cum], dtype=np.float64,\n",
    "    )\n",
    "    # Average normalized balanced accuracy overall\n",
    "    bal_accu_avg = cm_avg_all.diagonal().mean()\n",
    "\n",
    "    # Plot and save normalized confusion matrix, optional feature importance\n",
    "    fig = plt.figure()\n",
    "    plot_conf_matrix(cm_avg_all, classes=[\"low\", \"mid\", \"high\"])\n",
    "    if np.all(save_fig_name != None):\n",
    "        savepdf(fig, \"confusion_norm_%s\" % save_fig_name)\n",
    "    if not show_conf_matrix:\n",
    "        plt.close(fig)\n",
    "    if show_feat_impt:\n",
    "        plot_feat_impt(\n",
    "            feat_names,\n",
    "            impt_avg,\n",
    "            variances=None,\n",
    "            save_fig_name=save_fig_name,\n",
    "            combo_impt=False,\n",
    "        )\n",
    "\n",
    "    # Set output based on chosen classifier\n",
    "    if show_feat_impt:\n",
    "        return arr_norm_state_avg, std_err, impt_avg\n",
    "    else:\n",
    "        return arr_norm_state_avg, std_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparams(df, random_state, cust_folds=None, show_feat_impt=True):\n",
    "    \"\"\"\n",
    "    Apply hyperparameters for model evaluation.\n",
    "    \n",
    "    Relies on `model_eval' function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select for highest performing parameters (averaged across 10 folds)\n",
    "    df[\"augmentation_type\"] = [\n",
    "        str(i).split(\" \")[1] for i in df.loc[:, \"augmentation_type\"]\n",
    "    ]\n",
    "    feat_subset = [\"estimator\", \"augmentation_k\", \"augmentation_type\"]\n",
    "    df = df.drop_duplicates(subset=feat_subset, keep=\"first\")\n",
    "    df = df.sort_values(by=feat_subset, ascending=[False, True, True])\n",
    "    df.columns = [col.split(\"__\")[-1] for col in df.columns]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Placeholder lists for output\n",
    "    df[[\"score_avg\", \"score_list\", \"std_err\", \"impt_avg\"]] = np.nan\n",
    "    df[[\"score_avg\", \"score_list\", \"std_err\", \"impt_avg\"]] = df[\n",
    "        [\"score_avg\", \"score_list\", \"std_err\", \"impt_avg\"]\n",
    "    ].astype(object)\n",
    "\n",
    "    for row in df.index:\n",
    "        # Dictionary of classifier parameters, dropping nans\n",
    "        cls_kwargs = df.iloc[row, -7:-4].dropna().to_dict()\n",
    "        # Classifier function using dict\n",
    "        if df.loc[row, \"estimator\"] == \"SVC\":\n",
    "            classifier = SVC(kernel=\"rbf\", class_weight=\"balanced\", **cls_kwargs)\n",
    "        else:\n",
    "            classifier = RandomForestClassifier(class_weight=\"balanced\", **cls_kwargs)\n",
    "        # Make sure augmentation_type is a function\n",
    "        augmentation_type = df.augmentation_type[row]\n",
    "        dispatcher = {\n",
    "            \"random_augment\": random_augment,\n",
    "            \"unsupervised_augment\": unsupervised_augment,\n",
    "            \"supervised_augment\": supervised_augment,\n",
    "        }\n",
    "        if isinstance(augmentation_type, str):\n",
    "            augmentation_type = dispatcher[augmentation_type]\n",
    "        # Name figures using concatenated strings\n",
    "        save_fig_name = \"_\".join(map(str, list(df.loc[row, feat_subset])))\n",
    "\n",
    "        # Make parameter dict\n",
    "        params = {\n",
    "            \"classifier\": classifier,\n",
    "            \"augmentation_type\": augmentation_type,\n",
    "            \"augmentation_k\": df.augmentation_k[row],\n",
    "            \"random_state\": random_state,\n",
    "            \"cust_folds\": cust_folds,\n",
    "            \"save_fig_name\": save_fig_name,\n",
    "            \"show_feat_impt\": show_feat_impt,\n",
    "        }\n",
    "        if show_feat_impt:\n",
    "            score, err, impt = model_eval(**params)\n",
    "            df.score_list[row] = score\n",
    "            df.std_err[row] = err\n",
    "            df.impt_avg[row] = impt\n",
    "        else:\n",
    "            score, err = model_eval(**params)\n",
    "            df.score_list[row] = score\n",
    "            df.std_err[row] = err\n",
    "        df.score_avg[row] = np.average(df.score_list[row])\n",
    "    cols_report = [\n",
    "        \"estimator\",\n",
    "        \"augmentation_type\",\n",
    "        \"augmentation_k\",\n",
    "        \"score_avg\",\n",
    "        \"score_list\",\n",
    "        \"std_err\",\n",
    "    ]\n",
    "    if show_feat_impt:\n",
    "        cols_report = [cols_report] + \"impt_avg\"\n",
    "\n",
    "    return df[cols_report]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T01:42:56.770018Z",
     "start_time": "2020-09-14T01:42:52.872064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook functions.ipynb to script\n",
      "[NbConvertApp] Writing 36369 bytes to functions.py\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    !jupyter nbconvert --to script functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
